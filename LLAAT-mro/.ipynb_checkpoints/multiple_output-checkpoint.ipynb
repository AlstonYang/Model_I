{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import related package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import tensorflow package for modeling\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Min-max normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "## Plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## Initializing module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "## Copy module\n",
    "import copy\n",
    "\n",
    "## Used to calculate the training time\n",
    "import time\n",
    "\n",
    "## Set the GUP environment\n",
    "import os\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the display\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control memory usage space for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前設備： 0\n",
      "目前設備名： GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "## 查詢有無可用 GPU\n",
    "torch.cuda.is_available()\n",
    "## 查詢可用 GPU 的數量\n",
    "torch.cuda.device_count()\n",
    "##目前設備\n",
    "print(\"目前設備：\",torch.cuda.current_device())\n",
    "## 目前設備名\n",
    "print(\"目前設備名：\",torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out some info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cacl(pred_value, actual_value):\n",
    "    \n",
    "#     yo, loss, tape = network.forward()\n",
    "    accuracy = []\n",
    "\n",
    "    for i in range(pred_value.shape[1]):\n",
    "        \n",
    "        accuracy.append(torch.mean(torch.abs(pred_value[:,i] - actual_value[:,i])))\n",
    "        accuracy.append(torch.mean(torch.abs((pred_value[:,i] - actual_value[:,i]) / actual_value[:,i]))) \n",
    "        accuracy.append(torch.sqrt(torch.mean((pred_value[:,i] - actual_value[:,i])**2)))\n",
    "        \n",
    "        for error in range(2000,3001,1000):\n",
    "            correct_times = torch.nonzero(torch.abs(pred_value[:,i] - actual_value[:,i]) <= error)\n",
    "            accuracy.append(correct_times.shape[0]/pred_value.shape[0])   \n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(name, pred_value, actual_value,block_index):\n",
    "    \n",
    "    fig, ax = plt.subplots(2,2,figsize=(20,10), sharex=True, sharey=True)\n",
    "#     ax.set_xlim(0,pred_value.shape[0])  \n",
    "    \n",
    "    \n",
    "    for i in range(yo.shape[1]):\n",
    "        ax[i//2,i%2].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax[i//2,i%2].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax[i//2,i%2].plot(pred_value[:,i], label=\"LLAAT\")\n",
    "        ax[i//2,i%2].plot(actual_value[:,i], label=\"Actual\")\n",
    "        ax[i//2,i%2].set_title(\"Forecasted performance for l=%d\" %(i+1))\n",
    "        ax[i//2,i%2].legend()\n",
    "        \n",
    "        \n",
    "    #fig.text(0.5, 0, \"Stage of training\", ha='center', fontsize=20)\n",
    "    #fig.text(0, 0.5, \"Copper price value\", va='center', rotation='vertical')\n",
    "\n",
    "    \n",
    "    fig.suptitle(\"In the %s process in the M=%d window\"%(name, block_index))\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"In the %s process in the M=%d window.png\"%(name, block_index),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adopted_node(network,block_index):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20,5))\n",
    "#     ax.set_xticklabels([i for i in range(network.nb_node_acceptable.shape[0]+1)])\n",
    "    \n",
    "    ax.set_title(\"Total amount of adopted hidden nodes in the training process in the M=%d window\"%(block_index))\n",
    "    ax.plot(network.nb_node_acceptable,\"-o\")\n",
    "\n",
    "    ax.set_xlabel(\"Stage of training\")\n",
    "    ax.set_ylabel(\"Hidden nodes\")\n",
    "    \n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    fig.savefig(\"hidden nodes in the training process in the M=%d window\"%(block_index),dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_table(evaluation_results, block_index, name, performance, nb_step4, nb_step6_1, nb_step6_2, time,adopted_hidden_node):\n",
    "\n",
    "    \n",
    "    new_result = pd.DataFrame({\n",
    "\n",
    "        \"Window_index\":block_index,\n",
    "        \"Stage\":name,\n",
    "        \"MAE_1\" : performance[0].item(),\n",
    "        \"MAPE_1\" : \"%.2f\"%(performance[1]*100).item(),\n",
    "        \"RMSE_1\" : performance[2].item(),\n",
    "        \"Accuracy(2000)_1\" : [round(performance[3]*100,2)],\n",
    "        \"Accuracy(3000)_1\" : [round(performance[4]*100,2)],\n",
    "        \"MAE_2\" : performance[5].item(),\n",
    "        \"MAPE_2\" : \"%.2f\"%(performance[6]*100).item(),\n",
    "        \"RMSE_2\" : performance[7].item(),\n",
    "        \"Accuracy(2000)_2\" : [round(performance[8]*100,2)],\n",
    "        \"Accuracy(3000)_2\" : [round(performance[9]*100,2)],\n",
    "        \"MAE_3\" : performance[10].item(),\n",
    "        \"MAPE_3\" : \"%.2f\"%(performance[11]*100).item(),\n",
    "        \"RMSE_3\" : performance[12].item(),\n",
    "        \"Accuracy(2000)_3\" : [round(performance[13]*100,2)],\n",
    "        \"Accuracy(3000)_3\" : [round(performance[14]*100,2)],\n",
    "        \"MAE_4\" : performance[15].item(),\n",
    "        \"MAPE_4\" : \"%.2f\"%(performance[16]*100).item(),\n",
    "        \"RMSE_4\" : performance[17].item(),\n",
    "        \"Accuracy(2000)_4\" : [round(performance[18]*100,2)],\n",
    "        \"Accuracy(3000)_4\" : [round(performance[19]*100,2)],\n",
    "        \"Step4\":nb_step4,\n",
    "        \"Step6.1\":nb_step6_1,\n",
    "        \"Step6.2\":nb_step6_2,\n",
    "        \"Time\":time,\n",
    "        \"Adopted_hidden_node\":adopted_hidden_node\n",
    "    })\n",
    "\n",
    "    evaluation_results = evaluation_results.append(new_result, ignore_index=True)\n",
    "    \n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(network, nb_step4, nb_step6_1, nb_step6_2, x_test, y_test, start, end, block_index,evaluation_table_train,evaluation_table_test):\n",
    "\n",
    "    ## Training_Step\n",
    "    print(\"<<Training step>>\")\n",
    "    print(\"The training time(s):\",end - start)\n",
    "    yo, loss= network.forward()\n",
    "    \n",
    "    pre_train = yo.data.cpu()\n",
    "    true_train = network.y.data.cpu()\n",
    "    \n",
    "    pred_value_train = torch.FloatTensor(sc.inverse_transform(pre_train))\n",
    "    actual_value_train = torch.FloatTensor(sc.inverse_transform(true_train))\n",
    "    accuracy_train = accuracy_cacl(pred_value_train,actual_value_train)\n",
    "\n",
    "    ## Test_step\n",
    "#     print(\"<<Testing step>>\")\n",
    "    pred_value_test = torch.FloatTensor(sc.inverse_transform(network.forecast(x_test).data.cpu()))\n",
    "    accuracy_test = accuracy_cacl(pred_value_test, y_test)\n",
    "    \n",
    "    total_time = nb_step4 + nb_step6_1 + nb_step6_2\n",
    "    print(\"<<The percentage of each step>>\")\n",
    "    print(\"Step 4: %.2f%%\"%((nb_step4/total_time)*100))\n",
    "    print(\"Step 6.1: %.2f%%\"%((nb_step6_1/total_time)*100))\n",
    "    print(\"Step 6.2: %.2f%%\"%((nb_step6_2/total_time)*100))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"Total frequency of cramming occurrences:\",nb_step6_2)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"The amount of hidden node that be pruned:\",network.nb_node_pruned)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    adopted_hidden_node = network.nb_node_acceptable[-1].item()\n",
    "    print(\"The amount of adopted hidden nodes:\",adopted_hidden_node)\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in training step>>\")\n",
    "    print(\"The MAE for l = 1: %.2f\" %(accuracy_train[0]))\n",
    "    print(\"The MAPE for l = 1: %.2f%%\" %(accuracy_train[1]))\n",
    "    print(\"The RMSE for l = 1: %.2f\" %(accuracy_train[2]))\n",
    "    print(\"The accuracy(2000) for l = 1: %.2f%%\" %(accuracy_train[3]*100))\n",
    "    print(\"The accuracy(3000) for l = 1: %.2f%%\" %(accuracy_train[4]*100))\n",
    "    print(\"The MAE for l = 2: %.2f\" %(accuracy_train[5]))\n",
    "    print(\"The MAPE for l = 2: %.2f%%\" %(accuracy_train[6]))\n",
    "    print(\"The RMSE for l = 2: %.2f\" %(accuracy_train[7]))\n",
    "    print(\"The accuracy(2000) for l = 2: %.2f%%\" %(accuracy_train[8]*100))\n",
    "    print(\"The accuracy(3000) for l = 2: %.2f%%\" %(accuracy_train[9]*100))\n",
    "    print(\"The MAE for l = 3: %.2f\" %(accuracy_train[10]))\n",
    "    print(\"The MAPE for l = 3: %.2f%%\" %(accuracy_train[11]))\n",
    "    print(\"The RMSE for l = 3: %.2f\" %(accuracy_train[12]))\n",
    "    print(\"The accuracy(2000) for l = 3: %.2f%%\" %(accuracy_train[13]*100))\n",
    "    print(\"The accuracy(3000) for l = 3: %.2f%%\" %(accuracy_train[14]*100))\n",
    "    print(\"The MAE for l = 4: %.2f\" %(accuracy_train[15]))\n",
    "    print(\"The MAPE for l = 4: %.2f%%\" %(accuracy_train[16]))\n",
    "    print(\"The RMSE for l = 4: %.2f\" %(accuracy_train[17]))\n",
    "    print(\"The accuracy(2000) for l = 4: %.2f%%\" %(accuracy_train[18]*100))\n",
    "    print(\"The accuracy(3000) for l = 4: %.2f%%\" %(accuracy_train[19]*100))\n",
    "\n",
    "    print(\"-\"*60)\n",
    "    print(\"<<Accuracy in inferencing step>>\")\n",
    "    print(\"The MAE for l = 1: %.2f\" %(accuracy_test[0]))\n",
    "    print(\"The MAPE for l = 1: %.2f%%\" %(accuracy_test[1]))\n",
    "    print(\"The RMSE for l = 1: %.2f\" %(accuracy_test[2]))\n",
    "    print(\"The accuracy(2000) for l = 1: %.2f%%\" %(accuracy_test[3]*100))\n",
    "    print(\"The accuracy(3000) for l = 1: %.2f%%\" %(accuracy_test[4]*100))\n",
    "    print(\"The MAE for l = 2: %.2f\" %(accuracy_test[5]))\n",
    "    print(\"The MAPE for l = 2: %.2f%%\" %(accuracy_test[6]))\n",
    "    print(\"The RMSE for l = 2: %.2f\" %(accuracy_test[7]))\n",
    "    print(\"The accuracy(2000) for l = 2: %.2f%%\" %(accuracy_test[8]*100))\n",
    "    print(\"The accuracy(3000) for l = 2: %.2f%%\" %(accuracy_test[9]*100))\n",
    "    print(\"The MAE for l = 3: %.2f\" %(accuracy_test[10]))\n",
    "    print(\"The MAPE for l = 3: %.2f%%\" %(accuracy_test[11]))\n",
    "    print(\"The RMSE for l = 3: %.2f\" %(accuracy_test[12]))\n",
    "    print(\"The accuracy(2000) for l = 3: %.2f%%\" %(accuracy_test[13]*100))\n",
    "    print(\"The accuracy(3000) for l = 3: %.2f%%\" %(accuracy_test[14]*100))\n",
    "    print(\"The MAE for l = 4: %.2f\" %(accuracy_test[15]))\n",
    "    print(\"The MAPE for l = 4: %.2f%%\" %(accuracy_test[16]))\n",
    "    print(\"The RMSE for l = 4: %.2f\" %(accuracy_test[17]))\n",
    "    print(\"The accuracy(2000) for l = 4: %.2f%%\" %(accuracy_test[18]*100))\n",
    "    print(\"The accuracy(3000) for l = 4: %.2f%%\" %(accuracy_test[19]*100))\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    evaluation_table_train = evaluation_table(evaluation_table_train, block_index, \"Training\", accuracy_train,nb_step4, nb_step6_1, nb_step6_2, end - start,adopted_hidden_node)\n",
    "    evaluation_table_test = evaluation_table(evaluation_table_test, block_index, \"Inferencing\", accuracy_test,nb_step4, nb_step6_1, nb_step6_2, end - start,adopted_hidden_node)\n",
    "    \n",
    "    pre_LDSS = sc.inverse_transform(network.forecast(x_test).data.cpu())\n",
    "#     pd.DataFrame(pre_LDSS).to_csv(\"pre_LDSS.csv\", index=False)\n",
    "    \n",
    "    plot_result(\"training\",pred_value_train, actual_value_train,block_index)\n",
    "    plot_result(\"inferencing\",pred_value_test, y_test,block_index)\n",
    "    plot_adopted_node(network,block_index)\n",
    "    \n",
    "    return(evaluation_table_train, evaluation_table_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=4, futureWeek=4, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use min-max normalization to scale the data to the range from 1 to 0\n",
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design get_data() to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(futureWeek):\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = data[\"Date\"]\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data, futureWeek=futureWeek)\n",
    "\n",
    "\n",
    "    return (x_data, y_data)\n",
    "\n",
    "#     return (x_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializing(network, initial_x, initial_y):\n",
    "    print(\"Initializing module\")\n",
    "    ## Find each minimum output value y\n",
    "    min_y = torch.min(initial_y, axis=0)\n",
    "\n",
    "    ## Subtract min_y from each y\n",
    "    res_y = initial_y-min_y.values\n",
    "    \n",
    "    ## Use linear regression to find the initial W1,b1,Wo,bo\n",
    "    reg = LinearRegression().fit(initial_x, res_y)\n",
    "#     ## Set up the initial parameter of the network\n",
    "    network.linear1.weight = torch.nn.Parameter(torch.FloatTensor(reg.coef_).cuda())\n",
    "#     network.linear1.weight = network.linear1.weight.cuda()\n",
    "    network.linear1.bias = torch.nn.Parameter(torch.FloatTensor(reg.intercept_).cuda())\n",
    "    network.linear2.weight=torch.nn.Parameter(torch.FloatTensor([[1,0,0,0], [0,1,0,0],[0,0,1,0],[0,0,0,1]]).cuda())\n",
    "    network.linear2.bias = torch.nn.Parameter(torch.FloatTensor(min_y.values).cuda())\n",
    "    ## Set up the acceptable of the initial network as True\n",
    "    print(\"Initial權重\")\n",
    "    print(reg.coef_)\n",
    "    print(reg.intercept_)\n",
    "    network.acceptable =True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecting(network, x_train_scaled, y_train_scaled):\n",
    "    \n",
    "    print(\"<<Selecting module>>\")\n",
    "    loss = []\n",
    "    temp_network = copy.deepcopy(network)\n",
    "    \n",
    "    ## Put each data into network to calculate the loss value\n",
    "    for i in range(x_train_scaled.shape[0]):\n",
    "        temp_network.setData(x_train_scaled[i].reshape(1,-1), y_train_scaled[i].reshape(1,-1))\n",
    "        loss.append((temp_network.forward()[1].item(),i))\n",
    "\n",
    "#     ## Sort the data according to the loss value from smallest to largest, and save the data index in sorted_index\n",
    "    sorted_index = [sorted_data[1] for sorted_data in sorted(loss, key = lambda x:x[0])]\n",
    "    \n",
    "    \n",
    "    ## Print out some info for debug\n",
    "    print(\"The loss value of k:\",loss[sorted_index[0]])\n",
    "#     print(\"The second_loss value of k:\",loss[sorted_index[1]])\n",
    "    print(\"Selecting module finish!\")\n",
    "    \n",
    "    return sorted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    \n",
    "    print(\"<<Matching module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "    yo, loss = network.forward()\n",
    "    \n",
    "    if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "\n",
    "        network.acceptable = True\n",
    "        print(\"Matching(o) first finished - the network is acceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(10000):\n",
    "            \n",
    "            yo, loss = network.forward()\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            loss_pre = loss\n",
    "#             print(\"<前Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adadelta(loss)\n",
    "            yo, loss = network.forward()\n",
    "#             print(\"<後Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss <= loss_pre and torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.acceptable = True\n",
    "                print(\"Matching finished - the network is acceptable\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "            elif loss <= loss_pre:\n",
    "                \n",
    "#                 print(\"*1.2\")\n",
    "                times_enlarge+=1\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "\n",
    "            else:         \n",
    "\n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "                    print(\"Matching finished - the network is Unacceptable\")\n",
    "                    print(\"Number of enlarge:\",times_enlarge)\n",
    "                    print(\"Number of shrink:\",times_shrink)\n",
    "                    return(initial_network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "#                     print(\"*0.7\")\n",
    "                    # Restore the papameter of the network\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    times_shrink+=1\n",
    "                    network.learning_rate *= 0.7\n",
    "                \n",
    "        network.acceptable = False\n",
    "        print(\"Matching的第%d回合\"%(i+1))\n",
    "        print(\"Matching finished - the network is Unacceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(initial_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching for reorganizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_for_reorganizing(network):\n",
    "\n",
    "    times_enlarge=0\n",
    "    times_shrink=0\n",
    "    \n",
    "    print(\"<<Matching module for reorganizing>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "    network.acceptable = False\n",
    "    initial_network = copy.deepcopy(network)\n",
    "    yo, loss = network.forward()\n",
    "    \n",
    "    if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "\n",
    "        network.acceptable = True\n",
    "        print(\"Matching(o) first finished - the network is acceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(network)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        for i in range(500):\n",
    "            \n",
    "            yo, loss = network.forward()\n",
    "            network_pre = copy.deepcopy(network)\n",
    "            loss_pre = loss\n",
    "#             print(\"<前Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Backward and check the loss performance of the network with new learning rate\n",
    "            network.backward_Adadelta(loss)\n",
    "            yo, loss = network.forward()\n",
    "#             print(\"<後Loss>\",loss)\n",
    "#             print(network.state_dict())\n",
    "            # Confirm whether the loss value of the adjusted network is smaller than the current one\n",
    "            if loss <= loss_pre and torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "\n",
    "                # If true, multiply the learning rate by 1.2\n",
    "                network.acceptable = True\n",
    "                print(\"Matching finished(o) - the network is acceptable\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "                return(network)\n",
    "\n",
    "            elif loss <= loss_pre:\n",
    "                \n",
    "#                 print(\"*1.2\")\n",
    "                times_enlarge+=1\n",
    "                network.learning_rate *= 1.2\n",
    "\n",
    "\n",
    "            else:         \n",
    "\n",
    "                # Identify whether the current learning rate is less than the threshold\n",
    "                if network.learning_rate <= network.threshold_for_lr:\n",
    "\n",
    "                    # If true, set the acceptable of the network as false and return it\n",
    "                    network.acceptable = False\n",
    "                    print(\"Matching finished(o) - the network is Unacceptable\")\n",
    "                    print(\"Number of enlarge:\",times_enlarge)\n",
    "                    print(\"Number of shrink:\",times_shrink)\n",
    "                    return(initial_network)\n",
    "\n",
    "                # On the contrary, restore w and adjust the learning rate\n",
    "                else:\n",
    "#                     print(\"*0.7\")\n",
    "                    # Restore the papameter of the network\n",
    "                    network = copy.deepcopy(network_pre)\n",
    "                    times_shrink+=1\n",
    "                    network.learning_rate *= 0.7\n",
    "                \n",
    "        network.acceptable = False\n",
    "        print(\"Matching的第%d回合\"%(i+1))\n",
    "        print(\"Matching finished - the network is Unacceptable\")\n",
    "        print(\"Number of enlarge:\",times_enlarge)\n",
    "        print(\"Number of shrink:\",times_shrink)\n",
    "        return(initial_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cramming module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramming(network):\n",
    "    \n",
    "    torch.random.manual_seed(0)\n",
    "    print(\"<<Cramming module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    \n",
    "    ## Find unsatisfied data:K\n",
    "    yo, loss = network.forward()\n",
    "    undesired_index = torch.nonzero(torch.abs(yo-network.y) > network.threshold_for_error)\n",
    "\n",
    "    ## Print out the undesired_index for debug\n",
    "    print(\"The index of the undesired data:\",undesired_index)\n",
    "\n",
    "\n",
    "    # Unsatisfied situation\n",
    "    ## Find the index of the unsatisfied data\n",
    "    k_data_num = undesired_index[0][0]\n",
    "\n",
    "    undesired_data = torch.reshape(network.x[k_data_num,:], [1,-1])\n",
    "\n",
    "    ## Remove the data that does not meet the error term\n",
    "    left_data = network.x[:k_data_num,:]\n",
    "    right_data = network.x[k_data_num+1:,:]\n",
    "    remain_tensor = torch.cat([left_data, right_data], 0)\n",
    "\n",
    "\n",
    "    ## Use the random method to find out the gamma and zeta\n",
    "    while True:\n",
    "\n",
    "        ## Find m-vector gamma: r\n",
    "        ## Use the random method to generate the gamma that can make the conditions met\n",
    "        gamma = torch.rand(size=[1,network.x.shape[1]]).cuda()\n",
    "        subtract_undesired_data = torch.sub(remain_tensor, undesired_data)\n",
    "        matmul_value = torch.mm(gamma,torch.t(subtract_undesired_data))\n",
    "\n",
    "        if torch.all(matmul_value != 0):\n",
    "            break\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ## Find the tiny value: zeta\n",
    "        ## Use the random method to generate the zeta that can make the conditions met\n",
    "        zeta = torch.rand(size=[1]).cuda()\n",
    "\n",
    "        if torch.all(torch.mul(torch.add(zeta,matmul_value),torch.sub(zeta,matmul_value))<0):\n",
    "            break\n",
    "\n",
    "    for i in range(undesired_index.shape[0]):\n",
    "\n",
    "        k_l = undesired_index[i][1]\n",
    "#         print(\"The output node:\",k_l)\n",
    "        ## The weight of input layer to hidden layer I\n",
    "        w10 = gamma\n",
    "        w11 = gamma\n",
    "        w12 = gamma\n",
    "\n",
    "        W1_new = torch.cat([w10,w11,w12],0)\n",
    "#         print(\"W1_new.shape:\",W1_new.shape)\n",
    "\n",
    "        ## The bias of input layer to hidden layer I\n",
    "        matual_value = torch.mm(gamma,torch.t(undesired_data))\n",
    "\n",
    "        b10 = torch.sub(zeta,matual_value)\n",
    "        b11 = -1*matual_value\n",
    "        b12 = torch.sub(-1*zeta,matual_value)\n",
    "\n",
    "\n",
    "        b1_new = torch.reshape(torch.cat([b10,b11,b12],0),[3])\n",
    "\n",
    "#         print(\"b1_new\",b1_new)\n",
    "\n",
    "\n",
    "        ## The weight of hidden layer I to output layer\n",
    "        gap = network.y[k_data_num, k_l]-yo[k_data_num, k_l]\n",
    "#         print(\"gap:\",gap)\n",
    "\n",
    "        wo0_value = gap/zeta\n",
    "        wo1_value = (-2*gap)/zeta\n",
    "        wo2_value = gap/zeta\n",
    "\n",
    "        index = torch.tensor([[k_l]])\n",
    "\n",
    "        wo0 = torch.FloatTensor(torch.zeros(1, 4).scatter_(1, index, 1)).cuda() * wo0_value\n",
    "        wo1 = torch.FloatTensor(torch.zeros(1, 4).scatter_(1, index, 1)).cuda() * wo1_value\n",
    "        wo2 = torch.FloatTensor(torch.zeros(1, 4).scatter_(1, index, 1)).cuda() * wo2_value\n",
    "\n",
    "\n",
    "#         print(\"Wo0\",wo0_value)\n",
    "#         print(\"Wo1\",wo1_value)\n",
    "#         print(\"Wo2\",wo2_value)\n",
    "\n",
    "        Wo_new = torch.t(torch.cat([wo0,wo1,wo2],0))\n",
    "\n",
    "#         print(\"Wo_new.shape\",Wo_new.shape)\n",
    "\n",
    "        ## Add new neuroes to the network\n",
    "        network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight.data, W1_new]))\n",
    "        network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias.data, b1_new]))\n",
    "        network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight.data, Wo_new],1))\n",
    "#         print(network.state_dict())\n",
    "#         yo, loss = network.forward()\n",
    "\n",
    "#         print(torch.abs(network.y-yo))\n",
    "\n",
    "    yo, loss = network.forward()\n",
    "    ## Determine if cramming is successful and print out the corresponding information\n",
    "    if torch.all(torch.abs(yo[k_data_num,k_l]-network.y[k_data_num,k_l]) <= network.threshold_for_error):\n",
    "        network.acceptable = True \n",
    "        print(\"Cramming success!\")\n",
    "\n",
    "    else:\n",
    "        print(\"Cramming failed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizing(network):\n",
    "\n",
    "    print(\"<<Regularizing module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    ## Record the number of executions\n",
    "    times_enlarge = 0\n",
    "    times_shrink = 0\n",
    "    ## Set up the learning rate of the network\n",
    "    network.learning_rate = 1e-3\n",
    "\n",
    "    ## Set epoch to 100\n",
    "    for i in range(100):\n",
    "\n",
    "        ## Store the parameter of the network\n",
    "        network_pre = copy.deepcopy(network)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "        loss_pre = loss\n",
    "\n",
    "#         print(\"調整前的network\")\n",
    "#         print(\"<<變數>>\")\n",
    "#         print(network.state_dict())\n",
    "#         print(\"<<Loss值>>\")\n",
    "#         print(loss)\n",
    "#         print(\"差異\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "        \n",
    "        ## Backward operation to obtain w'\n",
    "        network.backward_Adadelta(loss)\n",
    "        yo, loss = network.forward(1e-3)\n",
    "#         print(\"調整後的network\")\n",
    "#         print(\"<<變數>>\")\n",
    "#         print(network.state_dict())\n",
    "#         print(\"<<Loss值>>\")\n",
    "#         print(loss)\n",
    "#         print(\"差異\")\n",
    "#         print(torch.abs(yo-network.y))\n",
    "         # Confirm whether the adjusted loss value is smaller than the current one\n",
    "        if loss <= loss_pre:\n",
    "            \n",
    "            ## Identify that all forecast value has met the error term\n",
    "            if torch.all(torch.abs(yo-network.y) <= network.threshold_for_error):\n",
    "                \n",
    "                ## If true, multiply the learning rate by 1.2\n",
    "#                 print(\"*1.2\")\n",
    "                network.learning_rate *= 1.2\n",
    "                times_enlarge += 1\n",
    "#                 print(\"Regularizing %d process - Enlarge\"%i)\n",
    "#                 print(\"第\\\"%d\\\"回合是成功執行regularizing\"%(i+1))\n",
    "#                 print(\"差異\")\n",
    "#                 print(torch.abs(yo-network.y))\n",
    "\n",
    "            else:\n",
    "\n",
    "                ## Else, restore w and end the process\n",
    "                network = copy.deepcopy(network_pre)\n",
    "                print(\"Regularizing結束-因為沒有顧好預測誤差\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "#                 print(\"Regularizing result: Unable to meet the error term\")\n",
    "                return(network)\n",
    "\n",
    "        # If the adjusted loss value is not smaller than the current one\n",
    "        else:\n",
    "\n",
    "            ## If the learning rate is greater than the threshold for learning rate\n",
    "            if network.learning_rate > network.threshold_for_lr:\n",
    "                \n",
    "                ## Restore the w and multiply the learning rate by 0.7\n",
    "                network = copy.deepcopy(network_pre)\n",
    "#                 print(\"*0.7\")\n",
    "                network.learning_rate *= 0.7\n",
    "                times_shrink += 1\n",
    "#                 print(\"把Learning rate變小\")\n",
    "#                 print(\"Regularizing %d process - Shrink\"%i)\n",
    "             ## If the learning rate is smaller than the threshold for learning rate\n",
    "            else:\n",
    "                \n",
    "                ## Restore the w\n",
    "                network = copy.deepcopy(network_pre)\n",
    "                print(\"Regularizing結束-Learning不能這麼小\")\n",
    "                print(\"Number of enlarge:\",times_enlarge)\n",
    "                print(\"Number of shrink:\",times_shrink)\n",
    "#                 print(\"Regularizing result: Less than the epsilon for the learning rate\")\n",
    "                return(network)\n",
    "\n",
    "    print(\"第\\\"%d\\\"回合Regularizing module完畢\"%(i+1))\n",
    "    print(\"Number of enlarge:\",times_enlarge)\n",
    "    print(\"Number of shrink:\",times_shrink)\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reorganizing module (Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganizing(network):\n",
    "    print(\"<<Reorganizing module>>\")\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "    ## Set up the k = 1, and p = the number of hidden node\n",
    "    k = 1\n",
    "#     p = network.W1.shape[1]\n",
    "    p = network.linear1.weight.data.shape[0]\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        ## If k > p, end of Process\n",
    "        if k > p or p==4:\n",
    "\n",
    "            print(\"Reorganizing result: The final number of neuro is \",p)\n",
    "            return(network)\n",
    "\n",
    "        ## Else, Process is ongoing\n",
    "        else:\n",
    "\n",
    "            ## Using the regularizing module to adjust the network\n",
    "            network = regularizing(network)\n",
    "            \n",
    "            ## Store the network and w\n",
    "            network_pre = copy.deepcopy(network)\n",
    "\n",
    "            ## Set up the acceptable of the network as false\n",
    "            network.acceptable = False\n",
    "\n",
    "            ## Ignore the K hidden node\n",
    "            network.linear1.weight = torch.nn.Parameter(torch.cat([network.linear1.weight[:k-1],network.linear1.weight[k:]],0))\n",
    "            network.linear1.bias = torch.nn.Parameter(torch.cat([network.linear1.bias[:k-1],network.linear1.bias[k:]]))\n",
    "            network.linear2.weight = torch.nn.Parameter(torch.cat([network.linear2.weight[:,:k-1],network.linear2.weight[:,k:]],1))\n",
    "\n",
    "            ## Using the matching module to adjust the network\n",
    "            network = matching_for_reorganizing(network)\n",
    "            \n",
    "            print(\"是不是可以不要你:\",network.acceptable)\n",
    "            \n",
    "            ## If the resulting network is acceptable, this means that the k hidden node can be removed\n",
    "            if network.acceptable:\n",
    "\n",
    "                print(\"Drop out the nero number: %d / %d\" %(k, p))\n",
    "                network.nb_node_pruned += 1\n",
    "                ## p--\n",
    "                p-=1\n",
    "\n",
    "            ## Else, it means that the k hidden node cannot be removed\n",
    "            else:\n",
    "                \n",
    "                ## Restore the network and w\n",
    "                network = network_pre\n",
    "                print(\"Cannot drop out the nero number: %d / %d\" %(k, p))\n",
    "                \n",
    "                ## k++\n",
    "                k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, nb_neuro, x_train_scaled, y_train_scaled):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(x_train_scaled.shape[1], nb_neuro).cuda()\n",
    "        self.linear2 = torch.nn.Linear(nb_neuro, y_train_scaled.shape[1]).cuda()\n",
    "        \n",
    "        \n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 0.07\n",
    "        self.threshold_for_lr = 1e-4\n",
    "        \n",
    "        # Input data \n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "        \n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-2\n",
    "        \n",
    "        # Whether the network is acceptable, default as False\n",
    "        self.acceptable = False\n",
    "        \n",
    "        # Some record for experiment\n",
    "        self.nb_node_pruned = 0\n",
    "        self.nb_node_acceptable=torch.IntTensor([nb_neuro])\n",
    "        \n",
    "    ## Forecast the test data\n",
    "    def forecast(self, x_test):\n",
    "    \n",
    "        x_test = torch.FloatTensor(x_test).cuda()\n",
    "        activation_value = self.linear1(x_test).clamp(min=0)\n",
    "        forecast_value = self.linear2(activation_value)\n",
    "       \n",
    "        return forecast_value\n",
    "\n",
    "    ## Reset the x and y data\n",
    "    def setData(self, x_train_scaled, y_train_scaled):\n",
    "        self.x = torch.FloatTensor(x_train_scaled).cuda()\n",
    "        self.y = torch.FloatTensor(y_train_scaled).cuda()\n",
    "    \n",
    "    ## Add the new data to the x and y data\n",
    "    def addData(self, new_x_train, new_y_train):\n",
    "\n",
    "        self.x = torch.cat([self.x, new_x_train.reshape(1,-1).cuda()],0)\n",
    "        self.y = torch.cat([self.y, new_y_train.reshape(1,-1).cuda()],0)\n",
    "    \n",
    "    ## forward operation\n",
    "    def forward(self, reg_strength=0):\n",
    "       \n",
    "        y1 = self.linear1(self.x).clamp(min=0)\n",
    "        yo = self.linear2(y1)\n",
    "\n",
    "        # performance measure\n",
    "        param_val= torch.sum(torch.pow(self.linear2.bias.data,2))+torch.sum(torch.pow(self.linear2.weight.data,2))+torch.sum(torch.pow(self.linear1.bias.data,2))+torch.sum(torch.pow(self.linear1.weight.data,2))\n",
    "        reg_term= reg_strength/((self.linear2.bias.data.shape[0]*(self.linear2.weight.data.shape[1]+1)) +(self.linear1.bias.data.shape[0]*(self.linear1.weight.data.shape[1]+1)))*param_val\n",
    "        loss = torch.nn.functional.mse_loss(yo,self.y)+reg_term\n",
    "        loss = loss.cuda()\n",
    "        return(yo, loss)\n",
    "\n",
    "    # backward operation\n",
    "    def backward_Adadelta(self,loss):    \n",
    "\n",
    "        optimizer = optim.Adadelta(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The <<1>> Block\n",
      "Initializing module\n",
      "Initial權重\n",
      "[[  0.45310813   1.2689383   -0.07516271   0.7545384    0.25834325\n",
      "    0.75845575   0.46586588   1.3790126   -1.3677397   -1.0765218\n",
      "    5.840321     0.2297505   -3.7934093    0.39298612  -0.16164947\n",
      "   -1.0163267   -1.1479506   -0.3698709 ]\n",
      " [  0.7737215   -0.49496508  -0.3882724    0.14156382  -1.7964969\n",
      "   -0.37444326  -1.0496526    1.1534762    0.1098735    1.0158751\n",
      "    0.4914682    1.0904509    1.457519     0.8232912   -0.22812697\n",
      "    0.17937332  -0.83382195   0.27945322]\n",
      " [ -1.757518     3.0850227    0.20149434  -3.8978837    2.9792056\n",
      "    1.2733893    0.12361309   0.39932638  -2.3409784   -2.9486117\n",
      "    5.2446623   -5.476268     0.6229651    0.71246785   1.2097054\n",
      "   -1.7741139   -0.5078775    0.9869914 ]\n",
      " [  0.6542824    1.078102     0.38694885  -2.5317125   -1.6396308\n",
      "    2.3836753   -0.9993675    3.2821834   -3.1852086   -1.756771\n",
      "    7.6005487    0.22283168 -12.917047     2.8058262    0.6309427\n",
      "   -1.8325394   -4.541271    -0.37289745]]\n",
      "[-0.5610503   0.09683427  0.40086797  3.4494815 ]\n",
      "<<Initializing後看一下差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "現在訓練到第幾筆資料: 20\n",
      "X 資料 torch.Size([60, 18])\n",
      "Y 資料 torch.Size([60, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0006851779762655497, 48)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引48，y= tensor([0.8356, 0.8355, 0.7936, 0.8166])\n",
      "目前模型的Data形狀 torch.Size([20, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9994, 0.9767, 0.9755, 0.9928],\n",
      "        [0.9767, 0.9755, 0.9928, 0.9790],\n",
      "        [0.9755, 0.9928, 0.9790, 0.9973],\n",
      "        [0.9928, 0.9790, 0.9973, 0.9834],\n",
      "        [0.9790, 0.9973, 0.9834, 0.9800],\n",
      "        [0.9973, 0.9834, 0.9800, 0.9673],\n",
      "        [0.9834, 0.9800, 0.9673, 0.9438],\n",
      "        [0.9800, 0.9673, 0.9438, 0.8890],\n",
      "        [0.9673, 0.9438, 0.8890, 0.9141],\n",
      "        [0.9438, 0.8890, 0.9141, 0.9367],\n",
      "        [0.8890, 0.9141, 0.9367, 0.9159],\n",
      "        [0.9141, 0.9367, 0.9159, 0.8533],\n",
      "        [0.9367, 0.9159, 0.8533, 0.8548],\n",
      "        [0.9159, 0.8533, 0.8548, 0.8454],\n",
      "        [0.8533, 0.8548, 0.8454, 0.7929],\n",
      "        [0.8548, 0.8454, 0.7929, 0.8065],\n",
      "        [0.8454, 0.7929, 0.8065, 0.8315],\n",
      "        [0.7929, 0.8065, 0.8315, 0.8036],\n",
      "        [0.8065, 0.8315, 0.8036, 0.8492],\n",
      "        [0.7929, 0.8541, 0.7929, 0.7929]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(    0.0000, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.05598092079162598\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 21\n",
      "X 資料 torch.Size([59, 18])\n",
      "Y 資料 torch.Size([59, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.001056202920153737, 47)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引47，y= tensor([0.8743, 0.8356, 0.8355, 0.7936])\n",
      "目前模型的Data形狀 torch.Size([21, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9994, 0.9767, 0.9755, 0.9928],\n",
      "        [0.9767, 0.9755, 0.9928, 0.9790],\n",
      "        [0.9755, 0.9928, 0.9790, 0.9973],\n",
      "        [0.9928, 0.9790, 0.9973, 0.9834],\n",
      "        [0.9790, 0.9973, 0.9834, 0.9800],\n",
      "        [0.9973, 0.9834, 0.9800, 0.9673],\n",
      "        [0.9834, 0.9800, 0.9673, 0.9438],\n",
      "        [0.9800, 0.9673, 0.9438, 0.8890],\n",
      "        [0.9673, 0.9438, 0.8890, 0.9141],\n",
      "        [0.9438, 0.8890, 0.9141, 0.9367],\n",
      "        [0.8890, 0.9141, 0.9367, 0.9159],\n",
      "        [0.9141, 0.9367, 0.9159, 0.8533],\n",
      "        [0.9367, 0.9159, 0.8533, 0.8548],\n",
      "        [0.9159, 0.8533, 0.8548, 0.8454],\n",
      "        [0.8533, 0.8548, 0.8454, 0.7929],\n",
      "        [0.8548, 0.8454, 0.7929, 0.8065],\n",
      "        [0.8454, 0.7929, 0.8065, 0.8315],\n",
      "        [0.7929, 0.8065, 0.8315, 0.8036],\n",
      "        [0.8065, 0.8315, 0.8036, 0.8492],\n",
      "        [0.7929, 0.8541, 0.7929, 0.7929],\n",
      "        [0.8984, 0.7929, 0.7929, 0.7929]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(    0.0001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.09000349044799805\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 22\n",
      "X 資料 torch.Size([58, 18])\n",
      "Y 資料 torch.Size([58, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.001960998633876443, 46)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引46，y= tensor([0.8617, 0.8743, 0.8356, 0.8355])\n",
      "目前模型的Data形狀 torch.Size([22, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9994, 0.9767, 0.9755, 0.9928],\n",
      "        [0.9767, 0.9755, 0.9928, 0.9790],\n",
      "        [0.9755, 0.9928, 0.9790, 0.9973],\n",
      "        [0.9928, 0.9790, 0.9973, 0.9834],\n",
      "        [0.9790, 0.9973, 0.9834, 0.9800],\n",
      "        [0.9973, 0.9834, 0.9800, 0.9673],\n",
      "        [0.9834, 0.9800, 0.9673, 0.9438],\n",
      "        [0.9800, 0.9673, 0.9438, 0.8890],\n",
      "        [0.9673, 0.9438, 0.8890, 0.9141],\n",
      "        [0.9438, 0.8890, 0.9141, 0.9367],\n",
      "        [0.8890, 0.9141, 0.9367, 0.9159],\n",
      "        [0.9141, 0.9367, 0.9159, 0.8533],\n",
      "        [0.9367, 0.9159, 0.8533, 0.8548],\n",
      "        [0.9159, 0.8533, 0.8548, 0.8454],\n",
      "        [0.8533, 0.8548, 0.8454, 0.7929],\n",
      "        [0.8548, 0.8454, 0.7929, 0.8065],\n",
      "        [0.8454, 0.7929, 0.8065, 0.8315],\n",
      "        [0.7929, 0.8065, 0.8315, 0.8036],\n",
      "        [0.8065, 0.8315, 0.8036, 0.8492],\n",
      "        [0.7929, 0.8541, 0.7929, 0.7929],\n",
      "        [0.8984, 0.7929, 0.7929, 0.7929],\n",
      "        [0.8478, 0.8110, 0.7929, 0.7929]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.12390542030334473\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 23\n",
      "X 資料 torch.Size([57, 18])\n",
      "Y 資料 torch.Size([57, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0023598975967615843, 54)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引54，y= tensor([0.7606, 0.7463, 0.7586, 0.7218])\n",
      "目前模型的Data形狀 torch.Size([23, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9994, 0.9767, 0.9755, 0.9928],\n",
      "        [0.9767, 0.9755, 0.9928, 0.9790],\n",
      "        [0.9755, 0.9928, 0.9790, 0.9973],\n",
      "        [0.9928, 0.9790, 0.9973, 0.9834],\n",
      "        [0.9790, 0.9973, 0.9834, 0.9800],\n",
      "        [0.9973, 0.9834, 0.9800, 0.9673],\n",
      "        [0.9834, 0.9800, 0.9673, 0.9438],\n",
      "        [0.9800, 0.9673, 0.9438, 0.8890],\n",
      "        [0.9673, 0.9438, 0.8890, 0.9141],\n",
      "        [0.9438, 0.8890, 0.9141, 0.9367],\n",
      "        [0.8890, 0.9141, 0.9367, 0.9159],\n",
      "        [0.9141, 0.9367, 0.9159, 0.8533],\n",
      "        [0.9367, 0.9159, 0.8533, 0.8548],\n",
      "        [0.9159, 0.8533, 0.8548, 0.8454],\n",
      "        [0.8533, 0.8548, 0.8454, 0.7929],\n",
      "        [0.8548, 0.8454, 0.7929, 0.8065],\n",
      "        [0.8454, 0.7929, 0.8065, 0.8315],\n",
      "        [0.7929, 0.8065, 0.8315, 0.8036],\n",
      "        [0.8065, 0.8315, 0.8036, 0.8492],\n",
      "        [0.7929, 0.8541, 0.7929, 0.7929],\n",
      "        [0.8984, 0.7929, 0.7929, 0.7929],\n",
      "        [0.8478, 0.8110, 0.7929, 0.7929],\n",
      "        [0.7929, 0.7929, 0.7929, 0.7929]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426],\n",
      "        [    0.0323,     0.0466,     0.0343,     0.0711]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426],\n",
      "        [    0.0323,     0.0466,     0.0343,     0.0711]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.1578664779663086\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 24\n",
      "X 資料 torch.Size([56, 18])\n",
      "Y 資料 torch.Size([56, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.002543102717027068, 46)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引46，y= tensor([0.8355, 0.7936, 0.8166, 0.7038])\n",
      "目前模型的Data形狀 torch.Size([24, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9994, 0.9767, 0.9755, 0.9928],\n",
      "        [0.9767, 0.9755, 0.9928, 0.9790],\n",
      "        [0.9755, 0.9928, 0.9790, 0.9973],\n",
      "        [0.9928, 0.9790, 0.9973, 0.9834],\n",
      "        [0.9790, 0.9973, 0.9834, 0.9800],\n",
      "        [0.9973, 0.9834, 0.9800, 0.9673],\n",
      "        [0.9834, 0.9800, 0.9673, 0.9438],\n",
      "        [0.9800, 0.9673, 0.9438, 0.8890],\n",
      "        [0.9673, 0.9438, 0.8890, 0.9141],\n",
      "        [0.9438, 0.8890, 0.9141, 0.9367],\n",
      "        [0.8890, 0.9141, 0.9367, 0.9159],\n",
      "        [0.9141, 0.9367, 0.9159, 0.8533],\n",
      "        [0.9367, 0.9159, 0.8533, 0.8548],\n",
      "        [0.9159, 0.8533, 0.8548, 0.8454],\n",
      "        [0.8533, 0.8548, 0.8454, 0.7929],\n",
      "        [0.8548, 0.8454, 0.7929, 0.8065],\n",
      "        [0.8454, 0.7929, 0.8065, 0.8315],\n",
      "        [0.7929, 0.8065, 0.8315, 0.8036],\n",
      "        [0.8065, 0.8315, 0.8036, 0.8492],\n",
      "        [0.7929, 0.8541, 0.7929, 0.7929],\n",
      "        [0.8984, 0.7929, 0.7929, 0.7929],\n",
      "        [0.8478, 0.8110, 0.7929, 0.7929],\n",
      "        [0.7929, 0.7929, 0.7929, 0.7929],\n",
      "        [0.7958, 0.8040, 0.7929, 0.7929]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426],\n",
      "        [    0.0323,     0.0466,     0.0343,     0.0711],\n",
      "        [    0.0396,     0.0104,     0.0238,     0.0890]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426],\n",
      "        [    0.0323,     0.0466,     0.0343,     0.0711],\n",
      "        [    0.0396,     0.0104,     0.0238,     0.0890]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
      "       dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.19164276123046875\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 25\n",
      "X 資料 torch.Size([55, 18])\n",
      "Y 資料 torch.Size([55, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.003811149625107646, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引11，y= tensor([0.8492, 0.8295, 0.8496, 0.8043])\n",
      "目前模型的Data形狀 torch.Size([25, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9994, 0.9767, 0.9755, 0.9928],\n",
      "        [0.9767, 0.9755, 0.9928, 0.9790],\n",
      "        [0.9755, 0.9928, 0.9790, 0.9973],\n",
      "        [0.9928, 0.9790, 0.9973, 0.9834],\n",
      "        [0.9790, 0.9973, 0.9834, 0.9800],\n",
      "        [0.9973, 0.9834, 0.9800, 0.9673],\n",
      "        [0.9834, 0.9800, 0.9673, 0.9438],\n",
      "        [0.9800, 0.9673, 0.9438, 0.8890],\n",
      "        [0.9673, 0.9438, 0.8890, 0.9141],\n",
      "        [0.9438, 0.8890, 0.9141, 0.9367],\n",
      "        [0.8890, 0.9141, 0.9367, 0.9159],\n",
      "        [0.9141, 0.9367, 0.9159, 0.8533],\n",
      "        [0.9367, 0.9159, 0.8533, 0.8548],\n",
      "        [0.9159, 0.8533, 0.8548, 0.8454],\n",
      "        [0.8533, 0.8548, 0.8454, 0.7929],\n",
      "        [0.8548, 0.8454, 0.7929, 0.8065],\n",
      "        [0.8454, 0.7929, 0.8065, 0.8315],\n",
      "        [0.7929, 0.8065, 0.8315, 0.8036],\n",
      "        [0.8065, 0.8315, 0.8036, 0.8492],\n",
      "        [0.7929, 0.8541, 0.7929, 0.7929],\n",
      "        [0.8984, 0.7929, 0.7929, 0.7929],\n",
      "        [0.8478, 0.8110, 0.7929, 0.7929],\n",
      "        [0.7929, 0.7929, 0.7929, 0.7929],\n",
      "        [0.7958, 0.8040, 0.7929, 0.7929],\n",
      "        [0.9021, 0.9223, 0.9105, 0.7929]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426],\n",
      "        [    0.0323,     0.0466,     0.0343,     0.0711],\n",
      "        [    0.0396,     0.0104,     0.0238,     0.0890],\n",
      "        [    0.0529,     0.0928,     0.0609,     0.0114]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426],\n",
      "        [    0.0323,     0.0466,     0.0343,     0.0711],\n",
      "        [    0.0396,     0.0104,     0.0238,     0.0890],\n",
      "        [    0.0529,     0.0928,     0.0609,     0.0114]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.22628569602966309\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 26\n",
      "X 資料 torch.Size([54, 18])\n",
      "Y 資料 torch.Size([54, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.005204004235565662, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.8094, 0.8200, 0.8145, 0.8400])\n",
      "目前模型的Data形狀 torch.Size([26, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9994, 0.9767, 0.9755, 0.9928],\n",
      "        [0.9767, 0.9755, 0.9928, 0.9790],\n",
      "        [0.9755, 0.9928, 0.9790, 0.9973],\n",
      "        [0.9928, 0.9790, 0.9973, 0.9834],\n",
      "        [0.9790, 0.9973, 0.9834, 0.9800],\n",
      "        [0.9973, 0.9834, 0.9800, 0.9673],\n",
      "        [0.9834, 0.9800, 0.9673, 0.9438],\n",
      "        [0.9800, 0.9673, 0.9438, 0.8890],\n",
      "        [0.9673, 0.9438, 0.8890, 0.9141],\n",
      "        [0.9438, 0.8890, 0.9141, 0.9367],\n",
      "        [0.8890, 0.9141, 0.9367, 0.9159],\n",
      "        [0.9141, 0.9367, 0.9159, 0.8533],\n",
      "        [0.9367, 0.9159, 0.8533, 0.8548],\n",
      "        [0.9159, 0.8533, 0.8548, 0.8454],\n",
      "        [0.8533, 0.8548, 0.8454, 0.7929],\n",
      "        [0.8548, 0.8454, 0.7929, 0.8065],\n",
      "        [0.8454, 0.7929, 0.8065, 0.8315],\n",
      "        [0.7929, 0.8065, 0.8315, 0.8036],\n",
      "        [0.8065, 0.8315, 0.8036, 0.8492],\n",
      "        [0.7929, 0.8541, 0.7929, 0.7929],\n",
      "        [0.8984, 0.7929, 0.7929, 0.7929],\n",
      "        [0.8478, 0.8110, 0.7929, 0.7929],\n",
      "        [0.7929, 0.7929, 0.7929, 0.7929],\n",
      "        [0.7958, 0.8040, 0.7929, 0.7929],\n",
      "        [0.9021, 0.9223, 0.9105, 0.7929],\n",
      "        [0.9423, 0.7980, 0.7929, 0.7929]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0000,     0.0000,     0.0000,     0.0000],\n",
      "        [    0.0428,     0.0186,     0.0007,     0.0238],\n",
      "        [    0.0241,     0.0428,     0.0426,     0.0007],\n",
      "        [    0.0138,     0.0633,     0.0428,     0.0426],\n",
      "        [    0.0323,     0.0466,     0.0343,     0.0711],\n",
      "        [    0.0396,     0.0104,     0.0238,     0.0890],\n",
      "        [    0.0529,     0.0928,     0.0609,     0.0114],\n",
      "        [    0.1328,     0.0220,     0.0216,     0.0472]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 73\n",
      "Number of shrink: 21\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0042,     0.0031,     0.0018,     0.0003],\n",
      "        [    0.0045,     0.0027,     0.0022,     0.0003],\n",
      "        [    0.0040,     0.0020,     0.0024,     0.0005],\n",
      "        [    0.0054,     0.0022,     0.0027,     0.0005],\n",
      "        [    0.0061,     0.0022,     0.0029,     0.0005],\n",
      "        [    0.0068,     0.0027,     0.0029,     0.0005],\n",
      "        [    0.0066,     0.0024,     0.0032,     0.0005],\n",
      "        [    0.0067,     0.0022,     0.0033,     0.0005],\n",
      "        [    0.0074,     0.0025,     0.0032,     0.0003],\n",
      "        [    0.0075,     0.0023,     0.0031,     0.0002],\n",
      "        [    0.0071,     0.0027,     0.0031,     0.0001],\n",
      "        [    0.0074,     0.0027,     0.0031,     0.0000],\n",
      "        [    0.0089,     0.0024,     0.0030,     0.0000],\n",
      "        [    0.0093,     0.0026,     0.0030,     0.0003],\n",
      "        [    0.0094,     0.0028,     0.0032,     0.0006],\n",
      "        [    0.0093,     0.0023,     0.0115,     0.0006],\n",
      "        [    0.0107,     0.0021,     0.0021,     0.0008],\n",
      "        [    0.0051,     0.0029,     0.0029,     0.0010],\n",
      "        [    0.0083,     0.0036,     0.0007,     0.0012],\n",
      "        [    0.0376,     0.0171,     0.0106,     0.0285],\n",
      "        [    0.0144,     0.0468,     0.0310,     0.0051],\n",
      "        [    0.0238,     0.0641,     0.0313,     0.0472],\n",
      "        [    0.0371,     0.0424,     0.0456,     0.0663],\n",
      "        [    0.0377,     0.0088,     0.0124,     0.0843],\n",
      "        [    0.0414,     0.0878,     0.0583,     0.0156],\n",
      "        [    0.1179,     0.0258,     0.0099,     0.0514]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0042,     0.0031,     0.0018,     0.0003],\n",
      "        [    0.0045,     0.0027,     0.0022,     0.0003],\n",
      "        [    0.0040,     0.0020,     0.0024,     0.0005],\n",
      "        [    0.0054,     0.0022,     0.0027,     0.0005],\n",
      "        [    0.0061,     0.0022,     0.0029,     0.0005],\n",
      "        [    0.0068,     0.0027,     0.0029,     0.0005],\n",
      "        [    0.0066,     0.0024,     0.0032,     0.0005],\n",
      "        [    0.0067,     0.0022,     0.0033,     0.0005],\n",
      "        [    0.0074,     0.0025,     0.0032,     0.0003],\n",
      "        [    0.0075,     0.0023,     0.0031,     0.0002],\n",
      "        [    0.0071,     0.0027,     0.0031,     0.0001],\n",
      "        [    0.0074,     0.0027,     0.0031,     0.0000],\n",
      "        [    0.0089,     0.0024,     0.0030,     0.0000],\n",
      "        [    0.0093,     0.0026,     0.0030,     0.0003],\n",
      "        [    0.0094,     0.0028,     0.0032,     0.0006],\n",
      "        [    0.0093,     0.0023,     0.0115,     0.0006],\n",
      "        [    0.0107,     0.0021,     0.0021,     0.0008],\n",
      "        [    0.0051,     0.0029,     0.0029,     0.0010],\n",
      "        [    0.0083,     0.0036,     0.0007,     0.0012],\n",
      "        [    0.0376,     0.0171,     0.0106,     0.0285],\n",
      "        [    0.0144,     0.0468,     0.0310,     0.0051],\n",
      "        [    0.0238,     0.0641,     0.0313,     0.0472],\n",
      "        [    0.0371,     0.0424,     0.0456,     0.0663],\n",
      "        [    0.0377,     0.0088,     0.0124,     0.0843],\n",
      "        [    0.0414,     0.0878,     0.0583,     0.0156],\n",
      "        [    0.1179,     0.0258,     0.0099,     0.0514]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.4600512981414795\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 27\n",
      "X 資料 torch.Size([53, 18])\n",
      "Y 資料 torch.Size([53, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.004698643926531076, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.8200, 0.8145, 0.8400, 0.8425])\n",
      "目前模型的Data形狀 torch.Size([27, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9952, 0.9736, 0.9737, 0.9930],\n",
      "        [0.9722, 0.9728, 0.9905, 0.9794],\n",
      "        [0.9715, 0.9908, 0.9766, 0.9977],\n",
      "        [0.9873, 0.9769, 0.9946, 0.9839],\n",
      "        [0.9730, 0.9951, 0.9805, 0.9805],\n",
      "        [0.9905, 0.9806, 0.9771, 0.9678],\n",
      "        [0.9767, 0.9776, 0.9642, 0.9443],\n",
      "        [0.9733, 0.9651, 0.9405, 0.8895],\n",
      "        [0.9599, 0.9413, 0.8858, 0.9144],\n",
      "        [0.9363, 0.8867, 0.9110, 0.9368],\n",
      "        [0.8819, 0.9114, 0.9335, 0.9158],\n",
      "        [0.9067, 0.9339, 0.9128, 0.8534],\n",
      "        [0.9278, 0.9135, 0.8503, 0.8547],\n",
      "        [0.9066, 0.8507, 0.8518, 0.8452],\n",
      "        [0.8440, 0.8519, 0.8422, 0.7923],\n",
      "        [0.8455, 0.8431, 0.8044, 0.8060],\n",
      "        [0.8348, 0.7908, 0.8044, 0.8308],\n",
      "        [0.7979, 0.8037, 0.8286, 0.8026],\n",
      "        [0.7982, 0.8279, 0.8043, 0.8480],\n",
      "        [0.7980, 0.8526, 0.8042, 0.7882],\n",
      "        [0.8887, 0.7889, 0.8045, 0.7885],\n",
      "        [0.8378, 0.8103, 0.8043, 0.7883],\n",
      "        [0.7977, 0.7887, 0.8042, 0.7881],\n",
      "        [0.7978, 0.8024, 0.8042, 0.7881],\n",
      "        [0.8906, 0.9173, 0.9079, 0.7887],\n",
      "        [0.9274, 0.7941, 0.8046, 0.7886],\n",
      "        [0.9479, 0.7890, 0.8048, 0.8658]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0042,     0.0031,     0.0018,     0.0003],\n",
      "        [    0.0045,     0.0027,     0.0022,     0.0003],\n",
      "        [    0.0040,     0.0020,     0.0024,     0.0005],\n",
      "        [    0.0054,     0.0022,     0.0027,     0.0005],\n",
      "        [    0.0061,     0.0022,     0.0029,     0.0005],\n",
      "        [    0.0068,     0.0027,     0.0029,     0.0005],\n",
      "        [    0.0066,     0.0024,     0.0032,     0.0005],\n",
      "        [    0.0067,     0.0022,     0.0033,     0.0005],\n",
      "        [    0.0074,     0.0025,     0.0032,     0.0003],\n",
      "        [    0.0075,     0.0023,     0.0031,     0.0002],\n",
      "        [    0.0071,     0.0027,     0.0031,     0.0001],\n",
      "        [    0.0074,     0.0027,     0.0031,     0.0000],\n",
      "        [    0.0089,     0.0024,     0.0030,     0.0000],\n",
      "        [    0.0093,     0.0026,     0.0030,     0.0003],\n",
      "        [    0.0094,     0.0028,     0.0032,     0.0006],\n",
      "        [    0.0093,     0.0023,     0.0115,     0.0006],\n",
      "        [    0.0107,     0.0021,     0.0021,     0.0008],\n",
      "        [    0.0051,     0.0029,     0.0029,     0.0010],\n",
      "        [    0.0083,     0.0036,     0.0007,     0.0012],\n",
      "        [    0.0376,     0.0171,     0.0106,     0.0285],\n",
      "        [    0.0144,     0.0468,     0.0310,     0.0051],\n",
      "        [    0.0238,     0.0641,     0.0313,     0.0472],\n",
      "        [    0.0371,     0.0424,     0.0456,     0.0663],\n",
      "        [    0.0377,     0.0088,     0.0124,     0.0843],\n",
      "        [    0.0414,     0.0878,     0.0583,     0.0156],\n",
      "        [    0.1179,     0.0258,     0.0099,     0.0514],\n",
      "        [    0.1279,     0.0254,     0.0352,     0.0232]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0008, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 40\n",
      "Number of shrink: 0\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0065,     0.0011,     0.0013,     0.0003],\n",
      "        [    0.0069,     0.0007,     0.0018,     0.0003],\n",
      "        [    0.0056,     0.0004,     0.0020,     0.0001],\n",
      "        [    0.0079,     0.0001,     0.0024,     0.0001],\n",
      "        [    0.0089,     0.0002,     0.0027,     0.0001],\n",
      "        [    0.0103,     0.0006,     0.0027,     0.0002],\n",
      "        [    0.0098,     0.0002,     0.0030,     0.0002],\n",
      "        [    0.0098,     0.0001,     0.0032,     0.0001],\n",
      "        [    0.0114,     0.0003,     0.0031,     0.0005],\n",
      "        [    0.0115,     0.0001,     0.0030,     0.0007],\n",
      "        [    0.0109,     0.0006,     0.0031,     0.0009],\n",
      "        [    0.0113,     0.0007,     0.0032,     0.0008],\n",
      "        [    0.0141,     0.0003,     0.0030,     0.0009],\n",
      "        [    0.0148,     0.0005,     0.0031,     0.0013],\n",
      "        [    0.0149,     0.0009,     0.0034,     0.0016],\n",
      "        [    0.0147,     0.0001,     0.0148,     0.0017],\n",
      "        [    0.0171,     0.0002,     0.0013,     0.0020],\n",
      "        [    0.0040,     0.0009,     0.0032,     0.0023],\n",
      "        [    0.0092,     0.0019,     0.0040,     0.0024],\n",
      "        [    0.0386,     0.0196,     0.0139,     0.0292],\n",
      "        [    0.0089,     0.0446,     0.0276,     0.0058],\n",
      "        [    0.0293,     0.0613,     0.0280,     0.0479],\n",
      "        [    0.0359,     0.0446,     0.0489,     0.0656],\n",
      "        [    0.0389,     0.0112,     0.0091,     0.0835],\n",
      "        [    0.0337,     0.0890,     0.0580,     0.0161],\n",
      "        [    0.1079,     0.0242,     0.0064,     0.0520],\n",
      "        [    0.1176,     0.0231,     0.0317,     0.0218]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0065,     0.0011,     0.0013,     0.0003],\n",
      "        [    0.0069,     0.0007,     0.0018,     0.0003],\n",
      "        [    0.0056,     0.0004,     0.0020,     0.0001],\n",
      "        [    0.0079,     0.0001,     0.0024,     0.0001],\n",
      "        [    0.0089,     0.0002,     0.0027,     0.0001],\n",
      "        [    0.0103,     0.0006,     0.0027,     0.0002],\n",
      "        [    0.0098,     0.0002,     0.0030,     0.0002],\n",
      "        [    0.0098,     0.0001,     0.0032,     0.0001],\n",
      "        [    0.0114,     0.0003,     0.0031,     0.0005],\n",
      "        [    0.0115,     0.0001,     0.0030,     0.0007],\n",
      "        [    0.0109,     0.0006,     0.0031,     0.0009],\n",
      "        [    0.0113,     0.0007,     0.0032,     0.0008],\n",
      "        [    0.0141,     0.0003,     0.0030,     0.0009],\n",
      "        [    0.0148,     0.0005,     0.0031,     0.0013],\n",
      "        [    0.0149,     0.0009,     0.0034,     0.0016],\n",
      "        [    0.0147,     0.0001,     0.0148,     0.0017],\n",
      "        [    0.0171,     0.0002,     0.0013,     0.0020],\n",
      "        [    0.0040,     0.0009,     0.0032,     0.0023],\n",
      "        [    0.0092,     0.0019,     0.0040,     0.0024],\n",
      "        [    0.0386,     0.0196,     0.0139,     0.0292],\n",
      "        [    0.0089,     0.0446,     0.0276,     0.0058],\n",
      "        [    0.0293,     0.0613,     0.0280,     0.0479],\n",
      "        [    0.0359,     0.0446,     0.0489,     0.0656],\n",
      "        [    0.0389,     0.0112,     0.0091,     0.0835],\n",
      "        [    0.0337,     0.0890,     0.0580,     0.0161],\n",
      "        [    0.1079,     0.0242,     0.0064,     0.0520],\n",
      "        [    0.1176,     0.0231,     0.0317,     0.0218]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.5801811218261719\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 28\n",
      "X 資料 torch.Size([52, 18])\n",
      "Y 資料 torch.Size([52, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0048364633694291115, 42)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引42，y= tensor([0.8833, 0.8617, 0.8743, 0.8356])\n",
      "目前模型的Data形狀 torch.Size([28, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9929, 0.9756, 0.9742, 0.9924],\n",
      "        [0.9698, 0.9749, 0.9909, 0.9787],\n",
      "        [0.9699, 0.9932, 0.9771, 0.9972],\n",
      "        [0.9848, 0.9792, 0.9949, 0.9833],\n",
      "        [0.9701, 0.9974, 0.9807, 0.9799],\n",
      "        [0.9869, 0.9828, 0.9773, 0.9671],\n",
      "        [0.9735, 0.9798, 0.9643, 0.9436],\n",
      "        [0.9702, 0.9674, 0.9406, 0.8888],\n",
      "        [0.9559, 0.9435, 0.8858, 0.9136],\n",
      "        [0.9323, 0.8889, 0.9111, 0.9360],\n",
      "        [0.8781, 0.9135, 0.9335, 0.9150],\n",
      "        [0.9028, 0.9360, 0.9127, 0.8526],\n",
      "        [0.9226, 0.9157, 0.8503, 0.8538],\n",
      "        [0.9011, 0.8529, 0.8517, 0.8442],\n",
      "        [0.8384, 0.8539, 0.8420, 0.7912],\n",
      "        [0.8400, 0.8453, 0.8077, 0.8049],\n",
      "        [0.8283, 0.7931, 0.8078, 0.8296],\n",
      "        [0.7969, 0.8057, 0.8284, 0.8014],\n",
      "        [0.7973, 0.8296, 0.8077, 0.8468],\n",
      "        [0.7971, 0.8551, 0.8075, 0.7875],\n",
      "        [0.8832, 0.7911, 0.8079, 0.7878],\n",
      "        [0.8323, 0.8131, 0.8077, 0.7876],\n",
      "        [0.7965, 0.7909, 0.8075, 0.7873],\n",
      "        [0.7966, 0.8048, 0.8075, 0.7874],\n",
      "        [0.8829, 0.9185, 0.9076, 0.7882],\n",
      "        [0.9174, 0.7958, 0.8080, 0.7880],\n",
      "        [0.9376, 0.7913, 0.8084, 0.8643],\n",
      "        [0.7965, 0.7909, 0.8075, 0.7873]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0065,     0.0011,     0.0013,     0.0003],\n",
      "        [    0.0069,     0.0007,     0.0018,     0.0003],\n",
      "        [    0.0056,     0.0004,     0.0020,     0.0001],\n",
      "        [    0.0079,     0.0001,     0.0024,     0.0001],\n",
      "        [    0.0089,     0.0002,     0.0027,     0.0001],\n",
      "        [    0.0103,     0.0006,     0.0027,     0.0002],\n",
      "        [    0.0098,     0.0002,     0.0030,     0.0002],\n",
      "        [    0.0098,     0.0001,     0.0032,     0.0001],\n",
      "        [    0.0114,     0.0003,     0.0031,     0.0005],\n",
      "        [    0.0115,     0.0001,     0.0030,     0.0007],\n",
      "        [    0.0109,     0.0006,     0.0031,     0.0009],\n",
      "        [    0.0113,     0.0007,     0.0032,     0.0008],\n",
      "        [    0.0141,     0.0003,     0.0030,     0.0009],\n",
      "        [    0.0148,     0.0005,     0.0031,     0.0013],\n",
      "        [    0.0149,     0.0009,     0.0034,     0.0016],\n",
      "        [    0.0147,     0.0001,     0.0148,     0.0017],\n",
      "        [    0.0171,     0.0002,     0.0013,     0.0020],\n",
      "        [    0.0040,     0.0009,     0.0032,     0.0023],\n",
      "        [    0.0092,     0.0019,     0.0040,     0.0024],\n",
      "        [    0.0386,     0.0196,     0.0139,     0.0292],\n",
      "        [    0.0089,     0.0446,     0.0276,     0.0058],\n",
      "        [    0.0293,     0.0613,     0.0280,     0.0479],\n",
      "        [    0.0359,     0.0446,     0.0489,     0.0656],\n",
      "        [    0.0389,     0.0112,     0.0091,     0.0835],\n",
      "        [    0.0337,     0.0890,     0.0580,     0.0161],\n",
      "        [    0.1079,     0.0242,     0.0064,     0.0520],\n",
      "        [    0.1176,     0.0231,     0.0317,     0.0218],\n",
      "        [    0.0868,     0.0708,     0.0668,     0.0483]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0065,     0.0011,     0.0013,     0.0003],\n",
      "        [    0.0069,     0.0007,     0.0018,     0.0003],\n",
      "        [    0.0056,     0.0004,     0.0020,     0.0001],\n",
      "        [    0.0079,     0.0001,     0.0024,     0.0001],\n",
      "        [    0.0089,     0.0002,     0.0027,     0.0001],\n",
      "        [    0.0103,     0.0006,     0.0027,     0.0002],\n",
      "        [    0.0098,     0.0002,     0.0030,     0.0002],\n",
      "        [    0.0098,     0.0001,     0.0032,     0.0001],\n",
      "        [    0.0114,     0.0003,     0.0031,     0.0005],\n",
      "        [    0.0115,     0.0001,     0.0030,     0.0007],\n",
      "        [    0.0109,     0.0006,     0.0031,     0.0009],\n",
      "        [    0.0113,     0.0007,     0.0032,     0.0008],\n",
      "        [    0.0141,     0.0003,     0.0030,     0.0009],\n",
      "        [    0.0148,     0.0005,     0.0031,     0.0013],\n",
      "        [    0.0149,     0.0009,     0.0034,     0.0016],\n",
      "        [    0.0147,     0.0001,     0.0148,     0.0017],\n",
      "        [    0.0171,     0.0002,     0.0013,     0.0020],\n",
      "        [    0.0040,     0.0009,     0.0032,     0.0023],\n",
      "        [    0.0092,     0.0019,     0.0040,     0.0024],\n",
      "        [    0.0386,     0.0196,     0.0139,     0.0292],\n",
      "        [    0.0089,     0.0446,     0.0276,     0.0058],\n",
      "        [    0.0293,     0.0613,     0.0280,     0.0479],\n",
      "        [    0.0359,     0.0446,     0.0489,     0.0656],\n",
      "        [    0.0389,     0.0112,     0.0091,     0.0835],\n",
      "        [    0.0337,     0.0890,     0.0580,     0.0161],\n",
      "        [    0.1079,     0.0242,     0.0064,     0.0520],\n",
      "        [    0.1176,     0.0231,     0.0317,     0.0218],\n",
      "        [    0.0868,     0.0708,     0.0668,     0.0483]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.6145279407501221\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 29\n",
      "X 資料 torch.Size([51, 18])\n",
      "Y 資料 torch.Size([51, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.00518034677952528, 14)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引14，y= tensor([0.8145, 0.8400, 0.8425, 0.8635])\n",
      "目前模型的Data形狀 torch.Size([29, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9929, 0.9756, 0.9742, 0.9924],\n",
      "        [0.9698, 0.9749, 0.9909, 0.9787],\n",
      "        [0.9699, 0.9932, 0.9771, 0.9972],\n",
      "        [0.9848, 0.9792, 0.9949, 0.9833],\n",
      "        [0.9701, 0.9974, 0.9807, 0.9799],\n",
      "        [0.9869, 0.9828, 0.9773, 0.9671],\n",
      "        [0.9735, 0.9798, 0.9643, 0.9436],\n",
      "        [0.9702, 0.9674, 0.9406, 0.8888],\n",
      "        [0.9559, 0.9435, 0.8858, 0.9136],\n",
      "        [0.9323, 0.8889, 0.9111, 0.9360],\n",
      "        [0.8781, 0.9135, 0.9335, 0.9150],\n",
      "        [0.9028, 0.9360, 0.9127, 0.8526],\n",
      "        [0.9226, 0.9157, 0.8503, 0.8538],\n",
      "        [0.9011, 0.8529, 0.8517, 0.8442],\n",
      "        [0.8384, 0.8539, 0.8420, 0.7912],\n",
      "        [0.8400, 0.8453, 0.8077, 0.8049],\n",
      "        [0.8283, 0.7931, 0.8078, 0.8296],\n",
      "        [0.7969, 0.8057, 0.8284, 0.8014],\n",
      "        [0.7973, 0.8296, 0.8077, 0.8468],\n",
      "        [0.7971, 0.8551, 0.8075, 0.7875],\n",
      "        [0.8832, 0.7911, 0.8079, 0.7878],\n",
      "        [0.8323, 0.8131, 0.8077, 0.7876],\n",
      "        [0.7965, 0.7909, 0.8075, 0.7873],\n",
      "        [0.7966, 0.8048, 0.8075, 0.7874],\n",
      "        [0.8829, 0.9185, 0.9076, 0.7882],\n",
      "        [0.9174, 0.7958, 0.8080, 0.7880],\n",
      "        [0.9376, 0.7913, 0.8084, 0.8643],\n",
      "        [0.7965, 0.7909, 0.8075, 0.7873],\n",
      "        [0.9453, 0.7914, 0.8084, 0.8726]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0065,     0.0011,     0.0013,     0.0003],\n",
      "        [    0.0069,     0.0007,     0.0018,     0.0003],\n",
      "        [    0.0056,     0.0004,     0.0020,     0.0001],\n",
      "        [    0.0079,     0.0001,     0.0024,     0.0001],\n",
      "        [    0.0089,     0.0002,     0.0027,     0.0001],\n",
      "        [    0.0103,     0.0006,     0.0027,     0.0002],\n",
      "        [    0.0098,     0.0002,     0.0030,     0.0002],\n",
      "        [    0.0098,     0.0001,     0.0032,     0.0001],\n",
      "        [    0.0114,     0.0003,     0.0031,     0.0005],\n",
      "        [    0.0115,     0.0001,     0.0030,     0.0007],\n",
      "        [    0.0109,     0.0006,     0.0031,     0.0009],\n",
      "        [    0.0113,     0.0007,     0.0032,     0.0008],\n",
      "        [    0.0141,     0.0003,     0.0030,     0.0009],\n",
      "        [    0.0148,     0.0005,     0.0031,     0.0013],\n",
      "        [    0.0149,     0.0009,     0.0034,     0.0016],\n",
      "        [    0.0147,     0.0001,     0.0148,     0.0017],\n",
      "        [    0.0171,     0.0002,     0.0013,     0.0020],\n",
      "        [    0.0040,     0.0009,     0.0032,     0.0023],\n",
      "        [    0.0092,     0.0019,     0.0040,     0.0024],\n",
      "        [    0.0386,     0.0196,     0.0139,     0.0292],\n",
      "        [    0.0089,     0.0446,     0.0276,     0.0058],\n",
      "        [    0.0293,     0.0613,     0.0280,     0.0479],\n",
      "        [    0.0359,     0.0446,     0.0489,     0.0656],\n",
      "        [    0.0389,     0.0112,     0.0091,     0.0835],\n",
      "        [    0.0337,     0.0890,     0.0580,     0.0161],\n",
      "        [    0.1079,     0.0242,     0.0064,     0.0520],\n",
      "        [    0.1176,     0.0231,     0.0317,     0.0218],\n",
      "        [    0.0868,     0.0708,     0.0668,     0.0483],\n",
      "        [    0.1308,     0.0487,     0.0341,     0.0091]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 46\n",
      "Number of shrink: 6\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[0.0109, 0.0065, 0.0008, 0.0011],\n",
      "        [0.0117, 0.0065, 0.0015, 0.0013],\n",
      "        [0.0097, 0.0058, 0.0018, 0.0012],\n",
      "        [0.0132, 0.0061, 0.0023, 0.0013],\n",
      "        [0.0148, 0.0062, 0.0027, 0.0014],\n",
      "        [0.0169, 0.0068, 0.0027, 0.0014],\n",
      "        [0.0161, 0.0068, 0.0032, 0.0015],\n",
      "        [0.0160, 0.0066, 0.0034, 0.0015],\n",
      "        [0.0187, 0.0066, 0.0033, 0.0018],\n",
      "        [0.0191, 0.0063, 0.0030, 0.0021],\n",
      "        [0.0179, 0.0068, 0.0033, 0.0023],\n",
      "        [0.0183, 0.0068, 0.0033, 0.0020],\n",
      "        [0.0226, 0.0058, 0.0031, 0.0022],\n",
      "        [0.0236, 0.0059, 0.0031, 0.0026],\n",
      "        [0.0235, 0.0062, 0.0035, 0.0012],\n",
      "        [0.0231, 0.0054, 0.0225, 0.0030],\n",
      "        [0.0266, 0.0098, 0.0089, 0.0033],\n",
      "        [0.0100, 0.0041, 0.0030, 0.0034],\n",
      "        [0.0030, 0.0059, 0.0118, 0.0034],\n",
      "        [0.0323, 0.0152, 0.0215, 0.0252],\n",
      "        [0.0007, 0.0328, 0.0199, 0.0018],\n",
      "        [0.0379, 0.0663, 0.0203, 0.0440],\n",
      "        [0.0421, 0.0562, 0.0566, 0.0696],\n",
      "        [0.0328, 0.0089, 0.0014, 0.0875],\n",
      "        [0.0234, 0.0851, 0.0583, 0.0120],\n",
      "        [0.0945, 0.0171, 0.0013, 0.0480],\n",
      "        [0.1036, 0.0113, 0.0238, 0.0207],\n",
      "        [0.0806, 0.0592, 0.0591, 0.0443],\n",
      "        [0.1169, 0.0368, 0.0262, 0.0080]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[0.0109, 0.0065, 0.0008, 0.0011],\n",
      "        [0.0117, 0.0065, 0.0015, 0.0013],\n",
      "        [0.0097, 0.0058, 0.0018, 0.0012],\n",
      "        [0.0132, 0.0061, 0.0023, 0.0013],\n",
      "        [0.0148, 0.0062, 0.0027, 0.0014],\n",
      "        [0.0169, 0.0068, 0.0027, 0.0014],\n",
      "        [0.0161, 0.0068, 0.0032, 0.0015],\n",
      "        [0.0160, 0.0066, 0.0034, 0.0015],\n",
      "        [0.0187, 0.0066, 0.0033, 0.0018],\n",
      "        [0.0191, 0.0063, 0.0030, 0.0021],\n",
      "        [0.0179, 0.0068, 0.0033, 0.0023],\n",
      "        [0.0183, 0.0068, 0.0033, 0.0020],\n",
      "        [0.0226, 0.0058, 0.0031, 0.0022],\n",
      "        [0.0236, 0.0059, 0.0031, 0.0026],\n",
      "        [0.0235, 0.0062, 0.0035, 0.0012],\n",
      "        [0.0231, 0.0054, 0.0225, 0.0030],\n",
      "        [0.0266, 0.0098, 0.0089, 0.0033],\n",
      "        [0.0100, 0.0041, 0.0030, 0.0034],\n",
      "        [0.0030, 0.0059, 0.0118, 0.0034],\n",
      "        [0.0323, 0.0152, 0.0215, 0.0252],\n",
      "        [0.0007, 0.0328, 0.0199, 0.0018],\n",
      "        [0.0379, 0.0663, 0.0203, 0.0440],\n",
      "        [0.0421, 0.0562, 0.0566, 0.0696],\n",
      "        [0.0328, 0.0089, 0.0014, 0.0875],\n",
      "        [0.0234, 0.0851, 0.0583, 0.0120],\n",
      "        [0.0945, 0.0171, 0.0013, 0.0480],\n",
      "        [0.1036, 0.0113, 0.0238, 0.0207],\n",
      "        [0.0806, 0.0592, 0.0591, 0.0443],\n",
      "        [0.1169, 0.0368, 0.0262, 0.0080]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 0.7629337310791016\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 30\n",
      "X 資料 torch.Size([50, 18])\n",
      "Y 資料 torch.Size([50, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.006199351977556944, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引11，y= tensor([0.8295, 0.8496, 0.8043, 0.8094])\n",
      "目前模型的Data形狀 torch.Size([30, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9885, 0.9702, 0.9748, 0.9916],\n",
      "        [0.9650, 0.9690, 0.9913, 0.9777],\n",
      "        [0.9658, 0.9869, 0.9773, 0.9961],\n",
      "        [0.9796, 0.9729, 0.9950, 0.9821],\n",
      "        [0.9643, 0.9910, 0.9807, 0.9786],\n",
      "        [0.9803, 0.9765, 0.9773, 0.9659],\n",
      "        [0.9672, 0.9733, 0.9642, 0.9423],\n",
      "        [0.9640, 0.9607, 0.9404, 0.8875],\n",
      "        [0.9486, 0.9372, 0.8857, 0.9123],\n",
      "        [0.9247, 0.8826, 0.9111, 0.9345],\n",
      "        [0.8711, 0.9073, 0.9334, 0.9136],\n",
      "        [0.8958, 0.9299, 0.9126, 0.8513],\n",
      "        [0.9141, 0.9101, 0.8503, 0.8525],\n",
      "        [0.8923, 0.8475, 0.8517, 0.8429],\n",
      "        [0.8299, 0.8485, 0.8419, 0.7917],\n",
      "        [0.8316, 0.8400, 0.8154, 0.8036],\n",
      "        [0.8188, 0.8026, 0.8155, 0.8282],\n",
      "        [0.8029, 0.8024, 0.8285, 0.8002],\n",
      "        [0.8035, 0.8256, 0.8154, 0.8458],\n",
      "        [0.8034, 0.8507, 0.8151, 0.7915],\n",
      "        [0.8750, 0.8028, 0.8156, 0.7918],\n",
      "        [0.8237, 0.8081, 0.8153, 0.7915],\n",
      "        [0.8027, 0.8025, 0.8152, 0.7913],\n",
      "        [0.8027, 0.8025, 0.8152, 0.7913],\n",
      "        [0.8726, 0.9147, 0.9079, 0.7923],\n",
      "        [0.9039, 0.8029, 0.8158, 0.7920],\n",
      "        [0.9236, 0.8032, 0.8163, 0.8632],\n",
      "        [0.8027, 0.8025, 0.8152, 0.7913],\n",
      "        [0.9314, 0.8032, 0.8163, 0.8715],\n",
      "        [0.8154, 1.0060, 0.8150, 0.8114]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[0.0109, 0.0065, 0.0008, 0.0011],\n",
      "        [0.0117, 0.0065, 0.0015, 0.0013],\n",
      "        [0.0097, 0.0058, 0.0018, 0.0012],\n",
      "        [0.0132, 0.0061, 0.0023, 0.0013],\n",
      "        [0.0148, 0.0062, 0.0027, 0.0014],\n",
      "        [0.0169, 0.0068, 0.0027, 0.0014],\n",
      "        [0.0161, 0.0068, 0.0032, 0.0015],\n",
      "        [0.0160, 0.0066, 0.0034, 0.0015],\n",
      "        [0.0187, 0.0066, 0.0033, 0.0018],\n",
      "        [0.0191, 0.0063, 0.0030, 0.0021],\n",
      "        [0.0179, 0.0068, 0.0033, 0.0023],\n",
      "        [0.0183, 0.0068, 0.0033, 0.0020],\n",
      "        [0.0226, 0.0058, 0.0031, 0.0022],\n",
      "        [0.0236, 0.0059, 0.0031, 0.0026],\n",
      "        [0.0235, 0.0062, 0.0035, 0.0012],\n",
      "        [0.0231, 0.0054, 0.0225, 0.0030],\n",
      "        [0.0266, 0.0098, 0.0089, 0.0033],\n",
      "        [0.0100, 0.0041, 0.0030, 0.0034],\n",
      "        [0.0030, 0.0059, 0.0118, 0.0034],\n",
      "        [0.0323, 0.0152, 0.0215, 0.0252],\n",
      "        [0.0007, 0.0328, 0.0199, 0.0018],\n",
      "        [0.0379, 0.0663, 0.0203, 0.0440],\n",
      "        [0.0421, 0.0562, 0.0566, 0.0696],\n",
      "        [0.0328, 0.0089, 0.0014, 0.0875],\n",
      "        [0.0234, 0.0851, 0.0583, 0.0120],\n",
      "        [0.0945, 0.0171, 0.0013, 0.0480],\n",
      "        [0.1036, 0.0113, 0.0238, 0.0207],\n",
      "        [0.0806, 0.0592, 0.0591, 0.0443],\n",
      "        [0.1169, 0.0368, 0.0262, 0.0080],\n",
      "        [0.0141, 0.1565, 0.0108, 0.0019]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 214\n",
      "Number of shrink: 92\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0023,     0.0041,     0.0001,     0.0011],\n",
      "        [    0.0018,     0.0034,     0.0005,     0.0014],\n",
      "        [    0.0119,     0.0010,     0.0007,     0.0002],\n",
      "        [    0.0010,     0.0038,     0.0009,     0.0012],\n",
      "        [    0.0031,     0.0066,     0.0017,     0.0016],\n",
      "        [    0.0109,     0.0101,     0.0028,     0.0022],\n",
      "        [    0.0054,     0.0094,     0.0029,     0.0017],\n",
      "        [    0.0019,     0.0073,     0.0027,     0.0011],\n",
      "        [    0.0121,     0.0111,     0.0033,     0.0023],\n",
      "        [    0.0139,     0.0080,     0.0024,     0.0029],\n",
      "        [    0.0097,     0.0108,     0.0037,     0.0029],\n",
      "        [    0.0090,     0.0122,     0.0042,     0.0021],\n",
      "        [    0.0237,     0.0165,     0.0045,     0.0034],\n",
      "        [    0.0278,     0.0183,     0.0053,     0.0044],\n",
      "        [    0.0263,     0.0221,     0.0072,     0.0004],\n",
      "        [    0.0243,     0.0187,     0.0229,     0.0046],\n",
      "        [    0.0279,     0.0231,     0.0094,     0.0055],\n",
      "        [    0.0246,     0.0087,     0.0079,     0.0060],\n",
      "        [    0.0111,     0.0151,     0.0125,     0.0066],\n",
      "        [    0.0176,     0.0012,     0.0217,     0.0235],\n",
      "        [    0.0010,     0.0185,     0.0190,     0.0001],\n",
      "        [    0.0378,     0.0591,     0.0200,     0.0424],\n",
      "        [    0.0567,     0.0688,     0.0569,     0.0712],\n",
      "        [    0.0182,     0.0214,     0.0012,     0.0892],\n",
      "        [    0.0040,     0.0521,     0.0490,     0.0103],\n",
      "        [    0.0515,     0.0033,     0.0018,     0.0465],\n",
      "        [    0.0552,     0.0044,     0.0226,     0.0142],\n",
      "        [    0.0660,     0.0466,     0.0589,     0.0426],\n",
      "        [    0.0702,     0.0206,     0.0249,     0.0018],\n",
      "        [    0.0066,     0.1178,     0.0101,     0.0025]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0023,     0.0041,     0.0001,     0.0011],\n",
      "        [    0.0018,     0.0034,     0.0005,     0.0014],\n",
      "        [    0.0119,     0.0010,     0.0007,     0.0002],\n",
      "        [    0.0010,     0.0038,     0.0009,     0.0012],\n",
      "        [    0.0031,     0.0066,     0.0017,     0.0016],\n",
      "        [    0.0109,     0.0101,     0.0028,     0.0022],\n",
      "        [    0.0054,     0.0094,     0.0029,     0.0017],\n",
      "        [    0.0019,     0.0073,     0.0027,     0.0011],\n",
      "        [    0.0121,     0.0111,     0.0033,     0.0023],\n",
      "        [    0.0139,     0.0080,     0.0024,     0.0029],\n",
      "        [    0.0097,     0.0108,     0.0037,     0.0029],\n",
      "        [    0.0090,     0.0122,     0.0042,     0.0021],\n",
      "        [    0.0237,     0.0165,     0.0045,     0.0034],\n",
      "        [    0.0278,     0.0183,     0.0053,     0.0044],\n",
      "        [    0.0263,     0.0221,     0.0072,     0.0004],\n",
      "        [    0.0243,     0.0187,     0.0229,     0.0046],\n",
      "        [    0.0279,     0.0231,     0.0094,     0.0055],\n",
      "        [    0.0246,     0.0087,     0.0079,     0.0060],\n",
      "        [    0.0111,     0.0151,     0.0125,     0.0066],\n",
      "        [    0.0176,     0.0012,     0.0217,     0.0235],\n",
      "        [    0.0010,     0.0185,     0.0190,     0.0001],\n",
      "        [    0.0378,     0.0591,     0.0200,     0.0424],\n",
      "        [    0.0567,     0.0688,     0.0569,     0.0712],\n",
      "        [    0.0182,     0.0214,     0.0012,     0.0892],\n",
      "        [    0.0040,     0.0521,     0.0490,     0.0103],\n",
      "        [    0.0515,     0.0033,     0.0018,     0.0465],\n",
      "        [    0.0552,     0.0044,     0.0226,     0.0142],\n",
      "        [    0.0660,     0.0466,     0.0589,     0.0426],\n",
      "        [    0.0702,     0.0206,     0.0249,     0.0018],\n",
      "        [    0.0066,     0.1178,     0.0101,     0.0025]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.441206932067871\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 31\n",
      "X 資料 torch.Size([49, 18])\n",
      "Y 資料 torch.Size([49, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.004635564051568508, 12)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引12，y= tensor([0.8043, 0.8094, 0.8200, 0.8145])\n",
      "目前模型的Data形狀 torch.Size([31, 4])\n",
      "<<預測值>>\n",
      "tensor([[1.0018, 0.9726, 0.9755, 0.9916],\n",
      "        [0.9785, 0.9721, 0.9923, 0.9777],\n",
      "        [0.9875, 0.9938, 0.9798, 0.9970],\n",
      "        [0.9938, 0.9753, 0.9964, 0.9822],\n",
      "        [0.9759, 0.9907, 0.9817, 0.9784],\n",
      "        [0.9864, 0.9733, 0.9772, 0.9652],\n",
      "        [0.9780, 0.9707, 0.9645, 0.9421],\n",
      "        [0.9781, 0.9600, 0.9411, 0.8879],\n",
      "        [0.9553, 0.9326, 0.8857, 0.9118],\n",
      "        [0.9299, 0.8809, 0.9117, 0.9338],\n",
      "        [0.8792, 0.9033, 0.9330, 0.9130],\n",
      "        [0.9051, 0.9245, 0.9117, 0.8512],\n",
      "        [0.9130, 0.8994, 0.8488, 0.8514],\n",
      "        [0.8881, 0.8351, 0.8495, 0.8411],\n",
      "        [0.8270, 0.8327, 0.8383, 0.7933],\n",
      "        [0.8304, 0.8267, 0.8157, 0.8019],\n",
      "        [0.8175, 0.8160, 0.8159, 0.8260],\n",
      "        [0.8175, 0.8152, 0.8236, 0.7976],\n",
      "        [0.8176, 0.8164, 0.8162, 0.8426],\n",
      "        [0.8181, 0.8366, 0.8153, 0.7932],\n",
      "        [0.8753, 0.8171, 0.8165, 0.7937],\n",
      "        [0.8238, 0.8153, 0.8156, 0.7931],\n",
      "        [0.8173, 0.8151, 0.8155, 0.7930],\n",
      "        [0.8173, 0.8151, 0.8155, 0.7930],\n",
      "        [0.8452, 0.8817, 0.8986, 0.7940],\n",
      "        [0.8610, 0.8166, 0.8163, 0.7935],\n",
      "        [0.8752, 0.8189, 0.8174, 0.8567],\n",
      "        [0.8173, 0.8151, 0.8155, 0.7930],\n",
      "        [0.8846, 0.8195, 0.8177, 0.8654],\n",
      "        [0.8229, 0.9674, 0.8144, 0.8070],\n",
      "        [0.8467, 0.9357, 0.8156, 0.8425]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0023,     0.0041,     0.0001,     0.0011],\n",
      "        [    0.0018,     0.0034,     0.0005,     0.0014],\n",
      "        [    0.0119,     0.0010,     0.0007,     0.0002],\n",
      "        [    0.0010,     0.0038,     0.0009,     0.0012],\n",
      "        [    0.0031,     0.0066,     0.0017,     0.0016],\n",
      "        [    0.0109,     0.0101,     0.0028,     0.0022],\n",
      "        [    0.0054,     0.0094,     0.0029,     0.0017],\n",
      "        [    0.0019,     0.0073,     0.0027,     0.0011],\n",
      "        [    0.0121,     0.0111,     0.0033,     0.0023],\n",
      "        [    0.0139,     0.0080,     0.0024,     0.0029],\n",
      "        [    0.0097,     0.0108,     0.0037,     0.0029],\n",
      "        [    0.0090,     0.0122,     0.0042,     0.0021],\n",
      "        [    0.0237,     0.0165,     0.0045,     0.0034],\n",
      "        [    0.0278,     0.0183,     0.0053,     0.0044],\n",
      "        [    0.0263,     0.0221,     0.0072,     0.0004],\n",
      "        [    0.0243,     0.0187,     0.0229,     0.0046],\n",
      "        [    0.0279,     0.0231,     0.0094,     0.0055],\n",
      "        [    0.0246,     0.0087,     0.0079,     0.0060],\n",
      "        [    0.0111,     0.0151,     0.0125,     0.0066],\n",
      "        [    0.0176,     0.0012,     0.0217,     0.0235],\n",
      "        [    0.0010,     0.0185,     0.0190,     0.0001],\n",
      "        [    0.0378,     0.0591,     0.0200,     0.0424],\n",
      "        [    0.0567,     0.0688,     0.0569,     0.0712],\n",
      "        [    0.0182,     0.0214,     0.0012,     0.0892],\n",
      "        [    0.0040,     0.0521,     0.0490,     0.0103],\n",
      "        [    0.0515,     0.0033,     0.0018,     0.0465],\n",
      "        [    0.0552,     0.0044,     0.0226,     0.0142],\n",
      "        [    0.0660,     0.0466,     0.0589,     0.0426],\n",
      "        [    0.0702,     0.0206,     0.0249,     0.0018],\n",
      "        [    0.0066,     0.1178,     0.0101,     0.0025],\n",
      "        [    0.0425,     0.1262,     0.0044,     0.0281]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished - the network is acceptable\n",
      "Number of enlarge: 41\n",
      "Number of shrink: 0\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.5658009052276611\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 32\n",
      "X 資料 torch.Size([48, 18])\n",
      "Y 資料 torch.Size([48, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0055381059646606445, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引11，y= tensor([0.8496, 0.8043, 0.8094, 0.8200])\n",
      "目前模型的Data形狀 torch.Size([32, 4])\n",
      "<<預測值>>\n",
      "tensor([[1.0035, 0.9697, 0.9761, 0.9924],\n",
      "        [0.9804, 0.9694, 0.9930, 0.9785],\n",
      "        [0.9902, 0.9918, 0.9806, 0.9982],\n",
      "        [0.9960, 0.9728, 0.9971, 0.9832],\n",
      "        [0.9780, 0.9880, 0.9824, 0.9794],\n",
      "        [0.9879, 0.9700, 0.9779, 0.9660],\n",
      "        [0.9801, 0.9677, 0.9652, 0.9431],\n",
      "        [0.9807, 0.9576, 0.9419, 0.8890],\n",
      "        [0.9571, 0.9293, 0.8864, 0.9127],\n",
      "        [0.9317, 0.8780, 0.9124, 0.9347],\n",
      "        [0.8812, 0.9001, 0.9337, 0.9139],\n",
      "        [0.9073, 0.9211, 0.9124, 0.8522],\n",
      "        [0.9142, 0.8952, 0.8494, 0.8521],\n",
      "        [0.8890, 0.8306, 0.8500, 0.8417],\n",
      "        [0.8280, 0.8277, 0.8387, 0.7931],\n",
      "        [0.8316, 0.8220, 0.8161, 0.8026],\n",
      "        [0.8176, 0.8146, 0.8162, 0.8265],\n",
      "        [0.8176, 0.8138, 0.8239, 0.7980],\n",
      "        [0.8177, 0.8151, 0.8165, 0.8428],\n",
      "        [0.8181, 0.8317, 0.8156, 0.7930],\n",
      "        [0.8768, 0.8160, 0.8168, 0.7936],\n",
      "        [0.8253, 0.8139, 0.8159, 0.7929],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8435, 0.8742, 0.8984, 0.7938],\n",
      "        [0.8583, 0.8152, 0.8165, 0.7933],\n",
      "        [0.8720, 0.8177, 0.8177, 0.8564],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8817, 0.8183, 0.8180, 0.8650],\n",
      "        [0.8228, 0.9592, 0.8146, 0.8068],\n",
      "        [0.8440, 0.9270, 0.8158, 0.8421],\n",
      "        [0.8382, 0.9170, 0.8169, 0.9162]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277],\n",
      "        [    0.0113,     0.1127,     0.0075,     0.0962]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277],\n",
      "        [    0.0113,     0.1127,     0.0075,     0.0962]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.6000308990478516\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 33\n",
      "X 資料 torch.Size([47, 18])\n",
      "Y 資料 torch.Size([47, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0060487110167741776, 37)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引37，y= tensor([0.9203, 0.8833, 0.8617, 0.8743])\n",
      "目前模型的Data形狀 torch.Size([33, 4])\n",
      "<<預測值>>\n",
      "tensor([[1.0035, 0.9697, 0.9761, 0.9924],\n",
      "        [0.9804, 0.9694, 0.9930, 0.9785],\n",
      "        [0.9902, 0.9918, 0.9806, 0.9982],\n",
      "        [0.9960, 0.9728, 0.9971, 0.9832],\n",
      "        [0.9780, 0.9880, 0.9824, 0.9794],\n",
      "        [0.9879, 0.9700, 0.9779, 0.9660],\n",
      "        [0.9801, 0.9677, 0.9652, 0.9431],\n",
      "        [0.9807, 0.9576, 0.9419, 0.8890],\n",
      "        [0.9571, 0.9293, 0.8864, 0.9127],\n",
      "        [0.9317, 0.8780, 0.9124, 0.9347],\n",
      "        [0.8812, 0.9001, 0.9337, 0.9139],\n",
      "        [0.9073, 0.9211, 0.9124, 0.8522],\n",
      "        [0.9142, 0.8952, 0.8494, 0.8521],\n",
      "        [0.8890, 0.8306, 0.8500, 0.8417],\n",
      "        [0.8280, 0.8277, 0.8387, 0.7931],\n",
      "        [0.8316, 0.8220, 0.8161, 0.8026],\n",
      "        [0.8176, 0.8146, 0.8162, 0.8265],\n",
      "        [0.8176, 0.8138, 0.8239, 0.7980],\n",
      "        [0.8177, 0.8151, 0.8165, 0.8428],\n",
      "        [0.8181, 0.8317, 0.8156, 0.7930],\n",
      "        [0.8768, 0.8160, 0.8168, 0.7936],\n",
      "        [0.8253, 0.8139, 0.8159, 0.7929],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8435, 0.8742, 0.8984, 0.7938],\n",
      "        [0.8583, 0.8152, 0.8165, 0.7933],\n",
      "        [0.8720, 0.8177, 0.8177, 0.8564],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8817, 0.8183, 0.8180, 0.8650],\n",
      "        [0.8228, 0.9592, 0.8146, 0.8068],\n",
      "        [0.8440, 0.9270, 0.8158, 0.8421],\n",
      "        [0.8382, 0.9170, 0.8169, 0.9162],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277],\n",
      "        [    0.0113,     0.1127,     0.0075,     0.0962],\n",
      "        [    0.1029,     0.0697,     0.0459,     0.0815]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0011, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277],\n",
      "        [    0.0113,     0.1127,     0.0075,     0.0962],\n",
      "        [    0.1029,     0.0697,     0.0459,     0.0815]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 1.63454270362854\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 34\n",
      "X 資料 torch.Size([46, 18])\n",
      "Y 資料 torch.Size([46, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.007213213015347719, 11)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引11，y= tensor([0.8400, 0.8425, 0.8635, 0.9530])\n",
      "目前模型的Data形狀 torch.Size([34, 4])\n",
      "<<預測值>>\n",
      "tensor([[1.0035, 0.9697, 0.9761, 0.9924],\n",
      "        [0.9804, 0.9694, 0.9930, 0.9785],\n",
      "        [0.9902, 0.9918, 0.9806, 0.9982],\n",
      "        [0.9960, 0.9728, 0.9971, 0.9832],\n",
      "        [0.9780, 0.9880, 0.9824, 0.9794],\n",
      "        [0.9879, 0.9700, 0.9779, 0.9660],\n",
      "        [0.9801, 0.9677, 0.9652, 0.9431],\n",
      "        [0.9807, 0.9576, 0.9419, 0.8890],\n",
      "        [0.9571, 0.9293, 0.8864, 0.9127],\n",
      "        [0.9317, 0.8780, 0.9124, 0.9347],\n",
      "        [0.8812, 0.9001, 0.9337, 0.9139],\n",
      "        [0.9073, 0.9211, 0.9124, 0.8522],\n",
      "        [0.9142, 0.8952, 0.8494, 0.8521],\n",
      "        [0.8890, 0.8306, 0.8500, 0.8417],\n",
      "        [0.8280, 0.8277, 0.8387, 0.7931],\n",
      "        [0.8316, 0.8220, 0.8161, 0.8026],\n",
      "        [0.8176, 0.8146, 0.8162, 0.8265],\n",
      "        [0.8176, 0.8138, 0.8239, 0.7980],\n",
      "        [0.8177, 0.8151, 0.8165, 0.8428],\n",
      "        [0.8181, 0.8317, 0.8156, 0.7930],\n",
      "        [0.8768, 0.8160, 0.8168, 0.7936],\n",
      "        [0.8253, 0.8139, 0.8159, 0.7929],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8435, 0.8742, 0.8984, 0.7938],\n",
      "        [0.8583, 0.8152, 0.8165, 0.7933],\n",
      "        [0.8720, 0.8177, 0.8177, 0.8564],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8817, 0.8183, 0.8180, 0.8650],\n",
      "        [0.8228, 0.9592, 0.8146, 0.8068],\n",
      "        [0.8440, 0.9270, 0.8158, 0.8421],\n",
      "        [0.8382, 0.9170, 0.8169, 0.9162],\n",
      "        [0.8174, 0.8136, 0.8157, 0.7929],\n",
      "        [0.8273, 0.8140, 0.8159, 0.7930]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277],\n",
      "        [    0.0113,     0.1127,     0.0075,     0.0962],\n",
      "        [    0.1029,     0.0697,     0.0459,     0.0815],\n",
      "        [    0.0127,     0.0285,     0.0476,     0.1600]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished - the network is Unacceptable\n",
      "Number of enlarge: 967\n",
      "Number of shrink: 501\n",
      "<<Matching後看一下差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277],\n",
      "        [    0.0113,     0.1127,     0.0075,     0.0962],\n",
      "        [    0.1029,     0.0697,     0.0459,     0.0815],\n",
      "        [    0.0127,     0.0285,     0.0476,     0.1600]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "<<Cramming module>>\n",
      "threshold_for_error: 0.118\n",
      "The index of the undesired data: tensor([[33,  3]], device='cuda:0')\n",
      "Cramming success!\n",
      "<<Cramming後看一下差異>>\n",
      "tensor([[    0.0041,     0.0070,     0.0006,     0.0003],\n",
      "        [    0.0037,     0.0061,     0.0002,     0.0005],\n",
      "        [    0.0147,     0.0009,     0.0016,     0.0009],\n",
      "        [    0.0033,     0.0062,     0.0001,     0.0002],\n",
      "        [    0.0010,     0.0093,     0.0009,     0.0006],\n",
      "        [    0.0093,     0.0134,     0.0021,     0.0013],\n",
      "        [    0.0033,     0.0123,     0.0022,     0.0007],\n",
      "        [    0.0007,     0.0098,     0.0019,     0.0001],\n",
      "        [    0.0102,     0.0145,     0.0026,     0.0014],\n",
      "        [    0.0121,     0.0110,     0.0017,     0.0019],\n",
      "        [    0.0078,     0.0140,     0.0030,     0.0020],\n",
      "        [    0.0068,     0.0156,     0.0035,     0.0012],\n",
      "        [    0.0224,     0.0207,     0.0039,     0.0027],\n",
      "        [    0.0269,     0.0228,     0.0048,     0.0037],\n",
      "        [    0.0254,     0.0271,     0.0068,     0.0003],\n",
      "        [    0.0232,     0.0234,     0.0232,     0.0040],\n",
      "        [    0.0278,     0.0218,     0.0097,     0.0050],\n",
      "        [    0.0247,     0.0073,     0.0077,     0.0056],\n",
      "        [    0.0111,     0.0164,     0.0128,     0.0064],\n",
      "        [    0.0176,     0.0038,     0.0219,     0.0237],\n",
      "        [    0.0025,     0.0197,     0.0187,     0.0001],\n",
      "        [    0.0364,     0.0604,     0.0198,     0.0425],\n",
      "        [    0.0568,     0.0673,     0.0571,     0.0711],\n",
      "        [    0.0181,     0.0200,     0.0009,     0.0890],\n",
      "        [    0.0057,     0.0447,     0.0488,     0.0105],\n",
      "        [    0.0488,     0.0047,     0.0020,     0.0467],\n",
      "        [    0.0520,     0.0032,     0.0224,     0.0138],\n",
      "        [    0.0659,     0.0480,     0.0586,     0.0428],\n",
      "        [    0.0672,     0.0217,     0.0246,     0.0015],\n",
      "        [    0.0067,     0.1096,     0.0103,     0.0027],\n",
      "        [    0.0398,     0.1175,     0.0041,     0.0277],\n",
      "        [    0.0113,     0.1127,     0.0075,     0.0962],\n",
      "        [    0.1029,     0.0697,     0.0459,     0.0815],\n",
      "        [    0.0127,     0.0285,     0.0476,     0.0000]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.118\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 70\n",
      "Number of shrink: 30\n",
      "<<Matching module for reorganizing>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished(o) - the network is acceptable\n",
      "Number of enlarge: 162\n",
      "Number of shrink: 75\n",
      "是不是可以不要你: True\n",
      "Drop out the nero number: 1 / 7\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.118\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 71\n",
      "Number of shrink: 29\n",
      "<<Matching module for reorganizing>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished(o) - the network is acceptable\n",
      "Number of enlarge: 31\n",
      "Number of shrink: 7\n",
      "是不是可以不要你: True\n",
      "Drop out the nero number: 1 / 6\n",
      "<<Regularizing module>>\n",
      "threshold_for_error: 0.118\n",
      "第\"100\"回合Regularizing module完畢\n",
      "Number of enlarge: 72\n",
      "Number of shrink: 28\n",
      "<<Matching module for reorganizing>>\n",
      "threshold_for_error: 0.118\n",
      "Matching finished(o) - the network is acceptable\n",
      "Number of enlarge: 149\n",
      "Number of shrink: 69\n",
      "是不是可以不要你: True\n",
      "Drop out the nero number: 1 / 5\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.494786739349365\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 35\n",
      "X 資料 torch.Size([45, 18])\n",
      "Y 資料 torch.Size([45, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0021601212210953236, 35)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引35，y= tensor([0.9552, 0.9203, 0.8833, 0.8617])\n",
      "目前模型的Data形狀 torch.Size([35, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9093, 0.8989, 0.8664, 0.9748],\n",
      "        [0.9270, 0.9160, 0.8749, 0.9664],\n",
      "        [0.9509, 0.9397, 0.8866, 0.9930],\n",
      "        [0.9492, 0.9377, 0.8856, 0.9804],\n",
      "        [0.9533, 0.9416, 0.8876, 0.9798],\n",
      "        [0.9400, 0.9284, 0.8810, 0.9627],\n",
      "        [0.9509, 0.9387, 0.8860, 0.9436],\n",
      "        [0.9561, 0.9427, 0.8879, 0.8921],\n",
      "        [0.9433, 0.9307, 0.8820, 0.9117],\n",
      "        [0.9429, 0.9307, 0.8820, 0.9345],\n",
      "        [0.9342, 0.9218, 0.8776, 0.9129],\n",
      "        [0.9265, 0.9131, 0.8731, 0.8493],\n",
      "        [0.9195, 0.9063, 0.8698, 0.8491],\n",
      "        [0.9116, 0.8984, 0.8658, 0.8373],\n",
      "        [0.9055, 0.8919, 0.8625, 0.8060],\n",
      "        [0.9048, 0.8912, 0.8622, 0.8066],\n",
      "        [0.8963, 0.8832, 0.8583, 0.8257],\n",
      "        [0.8776, 0.8646, 0.8490, 0.8041],\n",
      "        [0.8557, 0.8438, 0.8388, 0.8311],\n",
      "        [0.8867, 0.8736, 0.8534, 0.8012],\n",
      "        [0.9052, 0.8916, 0.8624, 0.8078],\n",
      "        [0.9156, 0.9018, 0.8674, 0.8101],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8726, 0.8598, 0.8466, 0.7964],\n",
      "        [0.8376, 0.8255, 0.8297, 0.7926],\n",
      "        [0.8425, 0.8304, 0.8321, 0.7946],\n",
      "        [0.8310, 0.8200, 0.8270, 0.8394],\n",
      "        [0.9107, 0.8970, 0.8651, 0.8132],\n",
      "        [0.8352, 0.8243, 0.8292, 0.8494],\n",
      "        [0.8458, 0.8335, 0.8336, 0.7935],\n",
      "        [0.8371, 0.8256, 0.8298, 0.8224],\n",
      "        [0.8433, 0.8331, 0.8337, 0.8992],\n",
      "        [0.9014, 0.8878, 0.8605, 0.8132],\n",
      "        [0.8281, 0.8163, 0.8251, 0.9494],\n",
      "        [0.8935, 0.8802, 0.8567, 0.8115]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.530242919921875\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 36\n",
      "X 資料 torch.Size([44, 18])\n",
      "Y 資料 torch.Size([44, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0025616767816245556, 31)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引31，y= tensor([0.9059, 0.9200, 0.9354, 0.9594])\n",
      "目前模型的Data形狀 torch.Size([36, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9093, 0.8989, 0.8664, 0.9748],\n",
      "        [0.9270, 0.9160, 0.8749, 0.9664],\n",
      "        [0.9509, 0.9397, 0.8866, 0.9930],\n",
      "        [0.9492, 0.9377, 0.8856, 0.9804],\n",
      "        [0.9533, 0.9416, 0.8876, 0.9798],\n",
      "        [0.9400, 0.9284, 0.8810, 0.9627],\n",
      "        [0.9509, 0.9387, 0.8860, 0.9436],\n",
      "        [0.9561, 0.9427, 0.8879, 0.8921],\n",
      "        [0.9433, 0.9307, 0.8820, 0.9117],\n",
      "        [0.9429, 0.9307, 0.8820, 0.9345],\n",
      "        [0.9342, 0.9218, 0.8776, 0.9129],\n",
      "        [0.9265, 0.9131, 0.8731, 0.8493],\n",
      "        [0.9195, 0.9063, 0.8698, 0.8491],\n",
      "        [0.9116, 0.8984, 0.8658, 0.8373],\n",
      "        [0.9055, 0.8919, 0.8625, 0.8060],\n",
      "        [0.9048, 0.8912, 0.8622, 0.8066],\n",
      "        [0.8963, 0.8832, 0.8583, 0.8257],\n",
      "        [0.8776, 0.8646, 0.8490, 0.8041],\n",
      "        [0.8557, 0.8438, 0.8388, 0.8311],\n",
      "        [0.8867, 0.8736, 0.8534, 0.8012],\n",
      "        [0.9052, 0.8916, 0.8624, 0.8078],\n",
      "        [0.9156, 0.9018, 0.8674, 0.8101],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8726, 0.8598, 0.8466, 0.7964],\n",
      "        [0.8376, 0.8255, 0.8297, 0.7926],\n",
      "        [0.8425, 0.8304, 0.8321, 0.7946],\n",
      "        [0.8310, 0.8200, 0.8270, 0.8394],\n",
      "        [0.9107, 0.8970, 0.8651, 0.8132],\n",
      "        [0.8352, 0.8243, 0.8292, 0.8494],\n",
      "        [0.8458, 0.8335, 0.8336, 0.7935],\n",
      "        [0.8371, 0.8256, 0.8298, 0.8224],\n",
      "        [0.8433, 0.8331, 0.8337, 0.8992],\n",
      "        [0.9014, 0.8878, 0.8605, 0.8132],\n",
      "        [0.8281, 0.8163, 0.8251, 0.9494],\n",
      "        [0.8935, 0.8802, 0.8567, 0.8115],\n",
      "        [0.8882, 0.8786, 0.8564, 1.0038]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.567026138305664\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 37\n",
      "X 資料 torch.Size([43, 18])\n",
      "Y 資料 torch.Size([43, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.004174068104475737, 40)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引40，y= tensor([0.7455, 0.7606, 0.7463, 0.7586])\n",
      "目前模型的Data形狀 torch.Size([37, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9093, 0.8989, 0.8664, 0.9748],\n",
      "        [0.9270, 0.9160, 0.8749, 0.9664],\n",
      "        [0.9509, 0.9397, 0.8866, 0.9930],\n",
      "        [0.9492, 0.9377, 0.8856, 0.9804],\n",
      "        [0.9533, 0.9416, 0.8876, 0.9798],\n",
      "        [0.9400, 0.9284, 0.8810, 0.9627],\n",
      "        [0.9509, 0.9387, 0.8860, 0.9436],\n",
      "        [0.9561, 0.9427, 0.8879, 0.8921],\n",
      "        [0.9433, 0.9307, 0.8820, 0.9117],\n",
      "        [0.9429, 0.9307, 0.8820, 0.9345],\n",
      "        [0.9342, 0.9218, 0.8776, 0.9129],\n",
      "        [0.9265, 0.9131, 0.8731, 0.8493],\n",
      "        [0.9195, 0.9063, 0.8698, 0.8491],\n",
      "        [0.9116, 0.8984, 0.8658, 0.8373],\n",
      "        [0.9055, 0.8919, 0.8625, 0.8060],\n",
      "        [0.9048, 0.8912, 0.8622, 0.8066],\n",
      "        [0.8963, 0.8832, 0.8583, 0.8257],\n",
      "        [0.8776, 0.8646, 0.8490, 0.8041],\n",
      "        [0.8557, 0.8438, 0.8388, 0.8311],\n",
      "        [0.8867, 0.8736, 0.8534, 0.8012],\n",
      "        [0.9052, 0.8916, 0.8624, 0.8078],\n",
      "        [0.9156, 0.9018, 0.8674, 0.8101],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8726, 0.8598, 0.8466, 0.7964],\n",
      "        [0.8376, 0.8255, 0.8297, 0.7926],\n",
      "        [0.8425, 0.8304, 0.8321, 0.7946],\n",
      "        [0.8310, 0.8200, 0.8270, 0.8394],\n",
      "        [0.9107, 0.8970, 0.8651, 0.8132],\n",
      "        [0.8352, 0.8243, 0.8292, 0.8494],\n",
      "        [0.8458, 0.8335, 0.8336, 0.7935],\n",
      "        [0.8371, 0.8256, 0.8298, 0.8224],\n",
      "        [0.8433, 0.8331, 0.8337, 0.8992],\n",
      "        [0.9014, 0.8878, 0.8605, 0.8132],\n",
      "        [0.8281, 0.8163, 0.8251, 0.9494],\n",
      "        [0.8935, 0.8802, 0.8567, 0.8115],\n",
      "        [0.8882, 0.8786, 0.8564, 1.0038],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0019, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.605058908462524\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 38\n",
      "X 資料 torch.Size([42, 18])\n",
      "Y 資料 torch.Size([42, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.004331283736974001, 39)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引39，y= tensor([0.7461, 0.7455, 0.7606, 0.7463])\n",
      "目前模型的Data形狀 torch.Size([38, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9093, 0.8989, 0.8664, 0.9748],\n",
      "        [0.9270, 0.9160, 0.8749, 0.9664],\n",
      "        [0.9509, 0.9397, 0.8866, 0.9930],\n",
      "        [0.9492, 0.9377, 0.8856, 0.9804],\n",
      "        [0.9533, 0.9416, 0.8876, 0.9798],\n",
      "        [0.9400, 0.9284, 0.8810, 0.9627],\n",
      "        [0.9509, 0.9387, 0.8860, 0.9436],\n",
      "        [0.9561, 0.9427, 0.8879, 0.8921],\n",
      "        [0.9433, 0.9307, 0.8820, 0.9117],\n",
      "        [0.9429, 0.9307, 0.8820, 0.9345],\n",
      "        [0.9342, 0.9218, 0.8776, 0.9129],\n",
      "        [0.9265, 0.9131, 0.8731, 0.8493],\n",
      "        [0.9195, 0.9063, 0.8698, 0.8491],\n",
      "        [0.9116, 0.8984, 0.8658, 0.8373],\n",
      "        [0.9055, 0.8919, 0.8625, 0.8060],\n",
      "        [0.9048, 0.8912, 0.8622, 0.8066],\n",
      "        [0.8963, 0.8832, 0.8583, 0.8257],\n",
      "        [0.8776, 0.8646, 0.8490, 0.8041],\n",
      "        [0.8557, 0.8438, 0.8388, 0.8311],\n",
      "        [0.8867, 0.8736, 0.8534, 0.8012],\n",
      "        [0.9052, 0.8916, 0.8624, 0.8078],\n",
      "        [0.9156, 0.9018, 0.8674, 0.8101],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8726, 0.8598, 0.8466, 0.7964],\n",
      "        [0.8376, 0.8255, 0.8297, 0.7926],\n",
      "        [0.8425, 0.8304, 0.8321, 0.7946],\n",
      "        [0.8310, 0.8200, 0.8270, 0.8394],\n",
      "        [0.9107, 0.8970, 0.8651, 0.8132],\n",
      "        [0.8352, 0.8243, 0.8292, 0.8494],\n",
      "        [0.8458, 0.8335, 0.8336, 0.7935],\n",
      "        [0.8371, 0.8256, 0.8298, 0.8224],\n",
      "        [0.8433, 0.8331, 0.8337, 0.8992],\n",
      "        [0.9014, 0.8878, 0.8605, 0.8132],\n",
      "        [0.8281, 0.8163, 0.8251, 0.9494],\n",
      "        [0.8935, 0.8802, 0.8567, 0.8115],\n",
      "        [0.8882, 0.8786, 0.8564, 1.0038],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299],\n",
      "        [    0.0810,     0.0699,     0.0640,     0.0422]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299],\n",
      "        [    0.0810,     0.0699,     0.0640,     0.0422]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.639281749725342\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 39\n",
      "X 資料 torch.Size([41, 18])\n",
      "Y 資料 torch.Size([41, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.0047974782064557076, 17)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引17，y= tensor([0.9669, 0.9521, 0.9018, 0.7079])\n",
      "目前模型的Data形狀 torch.Size([39, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9093, 0.8989, 0.8664, 0.9748],\n",
      "        [0.9270, 0.9160, 0.8749, 0.9664],\n",
      "        [0.9509, 0.9397, 0.8866, 0.9930],\n",
      "        [0.9492, 0.9377, 0.8856, 0.9804],\n",
      "        [0.9533, 0.9416, 0.8876, 0.9798],\n",
      "        [0.9400, 0.9284, 0.8810, 0.9627],\n",
      "        [0.9509, 0.9387, 0.8860, 0.9436],\n",
      "        [0.9561, 0.9427, 0.8879, 0.8921],\n",
      "        [0.9433, 0.9307, 0.8820, 0.9117],\n",
      "        [0.9429, 0.9307, 0.8820, 0.9345],\n",
      "        [0.9342, 0.9218, 0.8776, 0.9129],\n",
      "        [0.9265, 0.9131, 0.8731, 0.8493],\n",
      "        [0.9195, 0.9063, 0.8698, 0.8491],\n",
      "        [0.9116, 0.8984, 0.8658, 0.8373],\n",
      "        [0.9055, 0.8919, 0.8625, 0.8060],\n",
      "        [0.9048, 0.8912, 0.8622, 0.8066],\n",
      "        [0.8963, 0.8832, 0.8583, 0.8257],\n",
      "        [0.8776, 0.8646, 0.8490, 0.8041],\n",
      "        [0.8557, 0.8438, 0.8388, 0.8311],\n",
      "        [0.8867, 0.8736, 0.8534, 0.8012],\n",
      "        [0.9052, 0.8916, 0.8624, 0.8078],\n",
      "        [0.9156, 0.9018, 0.8674, 0.8101],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8726, 0.8598, 0.8466, 0.7964],\n",
      "        [0.8376, 0.8255, 0.8297, 0.7926],\n",
      "        [0.8425, 0.8304, 0.8321, 0.7946],\n",
      "        [0.8310, 0.8200, 0.8270, 0.8394],\n",
      "        [0.9107, 0.8970, 0.8651, 0.8132],\n",
      "        [0.8352, 0.8243, 0.8292, 0.8494],\n",
      "        [0.8458, 0.8335, 0.8336, 0.7935],\n",
      "        [0.8371, 0.8256, 0.8298, 0.8224],\n",
      "        [0.8433, 0.8331, 0.8337, 0.8992],\n",
      "        [0.9014, 0.8878, 0.8605, 0.8132],\n",
      "        [0.8281, 0.8163, 0.8251, 0.9494],\n",
      "        [0.8935, 0.8802, 0.8567, 0.8115],\n",
      "        [0.8882, 0.8786, 0.8564, 1.0038],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.9108, 0.8971, 0.8651, 0.8159]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299],\n",
      "        [    0.0810,     0.0699,     0.0640,     0.0422],\n",
      "        [    0.0561,     0.0550,     0.0367,     0.1080]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299],\n",
      "        [    0.0810,     0.0699,     0.0640,     0.0422],\n",
      "        [    0.0561,     0.0550,     0.0367,     0.1080]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.67285943031311\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 40\n",
      "X 資料 torch.Size([40, 18])\n",
      "Y 資料 torch.Size([40, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.005391975864768028, 32)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引32，y= tensor([0.9594, 0.9552, 0.9203, 0.8833])\n",
      "目前模型的Data形狀 torch.Size([40, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9093, 0.8989, 0.8664, 0.9748],\n",
      "        [0.9270, 0.9160, 0.8749, 0.9664],\n",
      "        [0.9509, 0.9397, 0.8866, 0.9930],\n",
      "        [0.9492, 0.9377, 0.8856, 0.9804],\n",
      "        [0.9533, 0.9416, 0.8876, 0.9798],\n",
      "        [0.9400, 0.9284, 0.8810, 0.9627],\n",
      "        [0.9509, 0.9387, 0.8860, 0.9436],\n",
      "        [0.9561, 0.9427, 0.8879, 0.8921],\n",
      "        [0.9433, 0.9307, 0.8820, 0.9117],\n",
      "        [0.9429, 0.9307, 0.8820, 0.9345],\n",
      "        [0.9342, 0.9218, 0.8776, 0.9129],\n",
      "        [0.9265, 0.9131, 0.8731, 0.8493],\n",
      "        [0.9195, 0.9063, 0.8698, 0.8491],\n",
      "        [0.9116, 0.8984, 0.8658, 0.8373],\n",
      "        [0.9055, 0.8919, 0.8625, 0.8060],\n",
      "        [0.9048, 0.8912, 0.8622, 0.8066],\n",
      "        [0.8963, 0.8832, 0.8583, 0.8257],\n",
      "        [0.8776, 0.8646, 0.8490, 0.8041],\n",
      "        [0.8557, 0.8438, 0.8388, 0.8311],\n",
      "        [0.8867, 0.8736, 0.8534, 0.8012],\n",
      "        [0.9052, 0.8916, 0.8624, 0.8078],\n",
      "        [0.9156, 0.9018, 0.8674, 0.8101],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8726, 0.8598, 0.8466, 0.7964],\n",
      "        [0.8376, 0.8255, 0.8297, 0.7926],\n",
      "        [0.8425, 0.8304, 0.8321, 0.7946],\n",
      "        [0.8310, 0.8200, 0.8270, 0.8394],\n",
      "        [0.9107, 0.8970, 0.8651, 0.8132],\n",
      "        [0.8352, 0.8243, 0.8292, 0.8494],\n",
      "        [0.8458, 0.8335, 0.8336, 0.7935],\n",
      "        [0.8371, 0.8256, 0.8298, 0.8224],\n",
      "        [0.8433, 0.8331, 0.8337, 0.8992],\n",
      "        [0.9014, 0.8878, 0.8605, 0.8132],\n",
      "        [0.8281, 0.8163, 0.8251, 0.9494],\n",
      "        [0.8935, 0.8802, 0.8567, 0.8115],\n",
      "        [0.8882, 0.8786, 0.8564, 1.0038],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.9108, 0.8971, 0.8651, 0.8159],\n",
      "        [0.8870, 0.8738, 0.8536, 0.8109]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299],\n",
      "        [    0.0810,     0.0699,     0.0640,     0.0422],\n",
      "        [    0.0561,     0.0550,     0.0367,     0.1080],\n",
      "        [    0.0724,     0.0814,     0.0667,     0.0724]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Reorganizing module>>\n",
      "threshold_for_error: 0.118\n",
      "Reorganizing result: The final number of neuro is  4\n",
      "<<Reorganizing後看一下差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299],\n",
      "        [    0.0810,     0.0699,     0.0640,     0.0422],\n",
      "        [    0.0561,     0.0550,     0.0367,     0.1080],\n",
      "        [    0.0724,     0.0814,     0.0667,     0.0724]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "看一下 hidden node\n",
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=torch.int32)\n",
      "使用裝置 cuda:0\n",
      "累計時間(s) 6.7075793743133545\n",
      "------------------------------------------------------------------------------------------\n",
      "現在訓練到第幾筆資料: 41\n",
      "X 資料 torch.Size([39, 18])\n",
      "Y 資料 torch.Size([39, 4])\n",
      "<<Selecting module>>\n",
      "The loss value of k: (0.008020123466849327, 10)\n",
      "Selecting module finish!\n",
      "現在要進去模型的數據，索引10，y= tensor([0.8036, 0.8492, 0.8295, 0.8496])\n",
      "目前模型的Data形狀 torch.Size([41, 4])\n",
      "<<預測值>>\n",
      "tensor([[0.9093, 0.8989, 0.8664, 0.9748],\n",
      "        [0.9270, 0.9160, 0.8749, 0.9664],\n",
      "        [0.9509, 0.9397, 0.8866, 0.9930],\n",
      "        [0.9492, 0.9377, 0.8856, 0.9804],\n",
      "        [0.9533, 0.9416, 0.8876, 0.9798],\n",
      "        [0.9400, 0.9284, 0.8810, 0.9627],\n",
      "        [0.9509, 0.9387, 0.8860, 0.9436],\n",
      "        [0.9561, 0.9427, 0.8879, 0.8921],\n",
      "        [0.9433, 0.9307, 0.8820, 0.9117],\n",
      "        [0.9429, 0.9307, 0.8820, 0.9345],\n",
      "        [0.9342, 0.9218, 0.8776, 0.9129],\n",
      "        [0.9265, 0.9131, 0.8731, 0.8493],\n",
      "        [0.9195, 0.9063, 0.8698, 0.8491],\n",
      "        [0.9116, 0.8984, 0.8658, 0.8373],\n",
      "        [0.9055, 0.8919, 0.8625, 0.8060],\n",
      "        [0.9048, 0.8912, 0.8622, 0.8066],\n",
      "        [0.8963, 0.8832, 0.8583, 0.8257],\n",
      "        [0.8776, 0.8646, 0.8490, 0.8041],\n",
      "        [0.8557, 0.8438, 0.8388, 0.8311],\n",
      "        [0.8867, 0.8736, 0.8534, 0.8012],\n",
      "        [0.9052, 0.8916, 0.8624, 0.8078],\n",
      "        [0.9156, 0.9018, 0.8674, 0.8101],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8726, 0.8598, 0.8466, 0.7964],\n",
      "        [0.8376, 0.8255, 0.8297, 0.7926],\n",
      "        [0.8425, 0.8304, 0.8321, 0.7946],\n",
      "        [0.8310, 0.8200, 0.8270, 0.8394],\n",
      "        [0.9107, 0.8970, 0.8651, 0.8132],\n",
      "        [0.8352, 0.8243, 0.8292, 0.8494],\n",
      "        [0.8458, 0.8335, 0.8336, 0.7935],\n",
      "        [0.8371, 0.8256, 0.8298, 0.8224],\n",
      "        [0.8433, 0.8331, 0.8337, 0.8992],\n",
      "        [0.9014, 0.8878, 0.8605, 0.8132],\n",
      "        [0.8281, 0.8163, 0.8251, 0.9494],\n",
      "        [0.8935, 0.8802, 0.8567, 0.8115],\n",
      "        [0.8882, 0.8786, 0.8564, 1.0038],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.8272, 0.8154, 0.8246, 0.7885],\n",
      "        [0.9108, 0.8971, 0.8651, 0.8159],\n",
      "        [0.8870, 0.8738, 0.8536, 0.8109],\n",
      "        [0.8472, 0.8392, 0.8370, 1.0229]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n",
      "<<差異>>\n",
      "tensor([[    0.0901,     0.0778,     0.1091,     0.0180],\n",
      "        [    0.0497,     0.0595,     0.1179,     0.0127],\n",
      "        [    0.0246,     0.0531,     0.0924,     0.0043],\n",
      "        [    0.0435,     0.0413,     0.1116,     0.0030],\n",
      "        [    0.0258,     0.0556,     0.0958,     0.0002],\n",
      "        [    0.0573,     0.0550,     0.0991,     0.0047],\n",
      "        [    0.0324,     0.0414,     0.0813,     0.0002],\n",
      "        [    0.0240,     0.0247,     0.0559,     0.0032],\n",
      "        [    0.0240,     0.0131,     0.0070,     0.0024],\n",
      "        [    0.0009,     0.0418,     0.0321,     0.0022],\n",
      "        [    0.0453,     0.0077,     0.0591,     0.0030],\n",
      "        [    0.0124,     0.0235,     0.0428,     0.0041],\n",
      "        [    0.0172,     0.0096,     0.0164,     0.0057],\n",
      "        [    0.0043,     0.0451,     0.0111,     0.0081],\n",
      "        [    0.0521,     0.0371,     0.0171,     0.0131],\n",
      "        [    0.0501,     0.0458,     0.0693,     0.0000],\n",
      "        [    0.0508,     0.0903,     0.0517,     0.0058],\n",
      "        [    0.0847,     0.0581,     0.0175,     0.0005],\n",
      "        [    0.0491,     0.0123,     0.0352,     0.0181],\n",
      "        [    0.0511,     0.0381,     0.0598,     0.0154],\n",
      "        [    0.0309,     0.0560,     0.0269,     0.0142],\n",
      "        [    0.0540,     0.0274,     0.0318,     0.0254],\n",
      "        [    0.0666,     0.0691,     0.0660,     0.0667],\n",
      "        [    0.0372,     0.0662,     0.0300,     0.0926],\n",
      "        [    0.0116,     0.0040,     0.0199,     0.0116],\n",
      "        [    0.0331,     0.0104,     0.0176,     0.0455],\n",
      "        [    0.0111,     0.0056,     0.0130,     0.0032],\n",
      "        [    0.0274,     0.0354,     0.0093,     0.0224],\n",
      "        [    0.0208,     0.0158,     0.0134,     0.0142],\n",
      "        [    0.0162,     0.0161,     0.0293,     0.0160],\n",
      "        [    0.0328,     0.0162,     0.0098,     0.0079],\n",
      "        [    0.0063,     0.0288,     0.0242,     0.0792],\n",
      "        [    0.0189,     0.0045,     0.0011,     0.0612],\n",
      "        [    0.0119,     0.0262,     0.0385,     0.0036],\n",
      "        [    0.0617,     0.0401,     0.0266,     0.0502],\n",
      "        [    0.0177,     0.0414,     0.0791,     0.0444],\n",
      "        [    0.0817,     0.0548,     0.0783,     0.0299],\n",
      "        [    0.0810,     0.0699,     0.0640,     0.0422],\n",
      "        [    0.0561,     0.0550,     0.0367,     0.1080],\n",
      "        [    0.0724,     0.0814,     0.0667,     0.0724],\n",
      "        [    0.0436,     0.0100,     0.0075,     0.1733]], device='cuda:0',\n",
      "       grad_fn=<AbsBackward>)\n",
      "threshold_for_error: 0.118\n",
      "Loss值\n",
      "tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "<<Matching module>>\n",
      "threshold_for_error: 0.118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-fa9fe96eacd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macceptable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<<Matching後看一下差異>>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fc2f7e8ab981>\u001b[0m in \u001b[0;36mmatching\u001b[0;34m(network)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#             print(network.state_dict())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Backward and check the loss performance of the network with new learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_Adadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0myo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#             print(\"<後Loss>\",loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-8cf2acf70b91>\u001b[0m in \u001b[0;36mbackward_Adadelta\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluation_table_train = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE_1\",\"MAPE_1\",\"RMSE_1\",\"Accuracy(2000)_1\",\"Accuracy(3000)_1\",\"MAE_2\",\"MAPE_2\",\"RMSE_2\",\"Accuracy(2000)_2\",\"Accuracy(3000)_2\",\"MAE_3\",\"MAPE_3\",\"RMSE_3\",\"Accuracy(2000)_3\",\"Accuracy(3000)_3\",\"MAE_4\",\"MAPE_4\",\"RMSE_4\",\"Accuracy(2000)_4\",\"Accuracy(3000)_4\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "evaluation_table_test = pd.DataFrame(columns=[\"Window_index\",\"Stage\",\"MAE_1\",\"MAPE_1\",\"RMSE_1\",\"Accuracy(2000)_1\",\"Accuracy(3000)_1\",\"MAE_2\",\"MAPE_2\",\"RMSE_2\",\"Accuracy(2000)_2\",\"Accuracy(3000)_2\",\"MAE_3\",\"MAPE_3\",\"RMSE_3\",\"Accuracy(2000)_3\",\"Accuracy(3000)_3\",\"MAE_4\",\"MAPE_4\",\"RMSE_4\",\"Accuracy(2000)_4\",\"Accuracy(3000)_4\",\"Step4\",\"Step6.1\",\"Step6.2\",\"Time\",\"Adopted_hidden_node\"])\n",
    "    \n",
    "x_data, y_data= get_data(4)\n",
    "x_data = sc.fit_transform(x_data)\n",
    "y_data = sc.fit_transform(y_data)\n",
    "threshold_for_error = 3000/(sc.data_max_-sc.data_min_)\n",
    "\n",
    "data = range(x_data.shape[0])\n",
    "# window_size => the length of training block\n",
    "window_size = 80\n",
    "# step_window => step size of each window\n",
    "step_window = 26\n",
    "# the split data\n",
    "splits = []\n",
    "\n",
    "## Moving window mechnism\n",
    "for i in range(window_size, len(data), step_window):\n",
    "    train = np.array(data[i-window_size:i])\n",
    "    test = np.array(data[i-window_size:i+step_window])\n",
    "#     test = np.array(data[i:i+step_window])\n",
    "    splits.append(('TRAIN:', train, 'TEST:', test))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i_block in range(len(splits)):\n",
    "# for i_block in range(-2,0,1):\n",
    "# for i_block in range(1):\n",
    "    block_start = time.time()\n",
    "    ## Record the number of each step\n",
    "    nb_step4 = 0\n",
    "    nb_step6_1 = 0\n",
    "    nb_step6_2 = 0\n",
    "    \n",
    "    print(\"The <<%d>> Block\" %(i_block+1))\n",
    "#     print(\"The <<%d>> Block\" %(len(splits)+i_block+1))\n",
    "#     print(\"The training block\\n\", y_data[splits[i_block][1]])\n",
    "#     print(\"The testing block\\n\", y_data[splits[i_block][3]])\n",
    "    \n",
    "    x_train = x_data[splits[i_block][1]]\n",
    "    x_test = x_data[splits[i_block][3]]\n",
    "    y_train = y_data[splits[i_block][1]]\n",
    "    y_test = y_data[splits[i_block][3]]\n",
    "    \n",
    "    x_train_scaled = torch.FloatTensor(x_train)\n",
    "    x_test_scaled = torch.FloatTensor(x_test)\n",
    "    y_train_scaled = torch.FloatTensor(y_train)\n",
    "    y_test = sc.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "#     if i_block == -2:\n",
    "    \n",
    "    \n",
    "    initial_x = torch.FloatTensor(x_train_scaled[10:29])\n",
    "    initial_y = torch.FloatTensor(y_train_scaled[10:29])\n",
    "    network = Network(1,initial_x,initial_y)\n",
    "    \n",
    "    x_train_scaled = torch.FloatTensor(np.concatenate([x_train_scaled[:10],x_train_scaled[30:]], axis=0))\n",
    "    y_train_scaled = torch.FloatTensor(np.concatenate([y_train_scaled[:10],y_train_scaled[30:]], axis=0))\n",
    "    \n",
    "# #         print(initial_x.shape[0])\n",
    "\n",
    "    network.nb_node_acceptable = torch.IntTensor([4 for _ in range(initial_x.shape[0])])\n",
    "    network.threshold_for_error = round(threshold_for_error[0],3)\n",
    "    network.nb_node_pruned = 0\n",
    "    \n",
    "    initializing(network, initial_x, initial_y)\n",
    "\n",
    "    print(\"<<Initializing後看一下差異>>\")\n",
    "    yo,loss = network.forward()\n",
    "    print(torch.abs(network.y-yo))\n",
    "    print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "  \n",
    "    \n",
    "    for i in range(int(x_train_scaled.shape[0]*0.9624)):\n",
    "#     for i in range(2):\n",
    "        \n",
    "#         if i_block == -2:\n",
    "       \n",
    "        print(\"現在訓練到第幾筆資料: %d\"%(i+x_train_scaled.shape[1]+2))\n",
    "        \n",
    "        print(\"X 資料\",x_train_scaled.shape)\n",
    "        print(\"Y 資料\",y_train_scaled.shape)\n",
    "        \n",
    "        sorted_index = selecting(network, x_train_scaled, y_train_scaled)\n",
    "        \n",
    "#         if i==0 and i_block > -2:\n",
    "       \n",
    "#             network.setData(x_train_scaled[sorted_index[0]].reshape(1,-1), y_train_scaled[sorted_index[0]].reshape(-1,1))\n",
    "            \n",
    "        ## Add new data for training\n",
    "        network.addData(x_train_scaled[sorted_index[0]], y_train_scaled[sorted_index[0]])\n",
    "        \n",
    "        print(\"現在要進去模型的數據，索引%d，y=\"%(sorted_index[0]),y_train_scaled[sorted_index[0]].data)\n",
    "        print(\"目前模型的Data形狀\",network.y.shape)\n",
    "        \n",
    "        x_train_scaled = np.delete(x_train_scaled, sorted_index[0], 0)\n",
    "        y_train_scaled = np.delete(y_train_scaled, sorted_index[0], 0)\n",
    "\n",
    "        \n",
    "        yo,loss = network.forward()\n",
    "        print(\"<<預測值>>\")\n",
    "        print(yo)\n",
    "        print(\"<<差異>>\")\n",
    "        print(torch.abs(yo-network.y))\n",
    "        print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        print(\"Loss值\")\n",
    "        print(loss)\n",
    "\n",
    "        pre_network = copy.deepcopy(network)\n",
    "        \n",
    "        if not torch.all(torch.abs(network.y-yo)<=network.threshold_for_error):\n",
    "\n",
    "            network.acceptable = False\n",
    "            network = matching(network)\n",
    "            \n",
    "            print(\"<<Matching後看一下差異>>\")\n",
    "            yo,loss = network.forward()\n",
    "            print(torch.abs(yo-network.y))\n",
    "            print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "            \n",
    "            if network.acceptable == False:\n",
    "                \n",
    "                network = copy.deepcopy(pre_network)\n",
    "                cramming(network)\n",
    "\n",
    "                if network.acceptable == False:\n",
    "                    sys.exit(\"資料出現兩筆以上不滿足\")  \n",
    "                \n",
    "                print(\"<<Cramming後看一下差異>>\")\n",
    "                yo,loss = network.forward()\n",
    "                print(torch.abs(yo-network.y))\n",
    "                print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "                nb_step6_2 += 1\n",
    "\n",
    "            else:\n",
    "                nb_step6_1 += 1\n",
    "\n",
    "        else:\n",
    "            nb_step4 += 1\n",
    "\n",
    "        network = reorganizing(network)\n",
    "        print(\"<<Reorganizing後看一下差異>>\")\n",
    "        yo,loss = network.forward()\n",
    "        print(torch.abs(yo-network.y))\n",
    "        print(\"threshold_for_error:\",network.threshold_for_error)\n",
    "        \n",
    "        network.nb_node_acceptable = torch.cat([network.nb_node_acceptable, torch.IntTensor([network.linear1.bias.data.shape[0]])],0)\n",
    "        print(\"看一下 hidden node\")\n",
    "        print(network.nb_node_acceptable)\n",
    "       \n",
    "        print(\"使用裝置\",(list(network.parameters())[0].device))\n",
    "        print(\"累計時間(s)\",time.time()-start)\n",
    "        print(\"-\"*90)\n",
    "\n",
    "    \n",
    "    block_end = time.time()\n",
    "    print(\"到第 %d 個區塊累積花費時間(s)\"%(i_block+1),block_end-block_start)\n",
    "#     print(\"到第 %d 個區塊累積花費時間(s)\"%(len(splits)+i_block+1),block_end-block_start)\n",
    "    print(\"<<The performance of %d block>>\"%(i_block+1))\n",
    "#     print(\"<<The performance of %d block>>\"%(len(splits)+i_block+1))\n",
    "#     validation(network, nb_step4, nb_step6_1, nb_step6_2, x_test_scaled, y_test, block_start, block_end,(len(splits)+i_block+1))\n",
    "    evaluation_table_train, evaluation_table_test = validation(network, nb_step4, nb_step6_1, nb_step6_2, x_test_scaled, y_test, block_start, block_end,i_block+1,evaluation_table_train,evaluation_table_test)\n",
    "\n",
    "    evaluation_table_train.to_csv(\"evaluation_table_train.csv\",index=False)\n",
    "    evaluation_table_test.to_csv(\"evaluation_table_inferencing.csv\",index=False)\n",
    "#     validation(network, nb_step4, nb_step6_1, nb_step6_2, x_test_scaled, y_test, block_start, block_end,(len(splits)+i_block+1))\n",
    "end = time.time()\n",
    "print(\"總計時間(s)\", end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
